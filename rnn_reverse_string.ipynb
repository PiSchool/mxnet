{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mxnet as mx\n",
    "\n",
    "ctx=mx.cpu(0)\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "from masked_bucket_io import MaskedBucketSentenceIter\n",
    "from xutils import read_content, load_vocab, sentence2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden=512\n",
    "embed_size=512\n",
    "batch_size=100\n",
    "dataset_size=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bucket_stride = 8\n",
    "buckets = []\n",
    "for i in range(8, 128, bucket_stride):\n",
    "    for j in range(8, 128, bucket_stride):\n",
    "        buckets.append((i, j))\n",
    "\n",
    "bos_word = '<s>'\n",
    "eos_word = '</s>'\n",
    "unk_word = '<unk>'\n",
    "special_words = {unk_word: 1, bos_word: 2, eos_word: 3}\n",
    "\n",
    "train_source = 'english'\n",
    "train_target = 'italian'\n",
    "source_vocab_path = train_source + \".pkl\"\n",
    "target_vocab_path = train_target + \".pkl\"\n",
    "\n",
    "source_vocab = load_vocab(source_vocab_path, special_words)\n",
    "inverted_source_vocab= {source_vocab[word]:word for word in source_vocab}\n",
    "inverted_source_vocab[0]=''\n",
    "\n",
    "target_vocab = load_vocab(target_vocab_path, special_words)\n",
    "inverted_target_vocab = {target_vocab[word]:word for word in target_vocab}\n",
    "inverted_target_vocab[0]=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = MaskedBucketSentenceIter(\n",
    "    train_source,\n",
    "    train_target,\n",
    "    source_vocab,\n",
    "    target_vocab,\n",
    "    buckets,\n",
    "    batch_size,\n",
    "    text2id=sentence2id,\n",
    "    read_content=read_content,\n",
    "    max_read_sample=100000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.reset()\n",
    "item=data_train.next()\n",
    "print(item)\n",
    "for i in range(batch_size):\n",
    "    print(\"source\")\n",
    "    print([inverted_source_vocab[int(i.asnumpy())] for i in item.data[0][i]])\n",
    "    print(\"mask\")\n",
    "    print([int(i.asnumpy()) for i in item.data[1][i]])\n",
    "    print(\"target\")\n",
    "    print([inverted_target_vocab[int(i.asnumpy())] for i in item.data[2][i]])\n",
    "    print(\"label\")\n",
    "    print([inverted_target_vocab[int(i.asnumpy())] for i in item.label[0][i]])\n",
    "    print()\n",
    "data_train.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s2s_unroll(\n",
    "    source_len,\n",
    "    target_len,\n",
    "    input_names,\n",
    "    output_names,\n",
    "    source_vocab_size,\n",
    "    target_vocab_size,\n",
    "    **kwargs):\n",
    "\n",
    "    source = mx.sym.Variable('source')\n",
    "    target = mx.sym.Variable('target')\n",
    "    label = mx.sym.Variable('target_softmax_label')\n",
    "\n",
    "    source_embed = mx.sym.Embedding(\n",
    "        data=source,\n",
    "        input_dim=source_vocab_size,\n",
    "        output_dim=embed_size,\n",
    "        weight= mx.sym.Variable('source_embed_weight')\n",
    "    )\n",
    "    \n",
    "    #source_vector_of_words = mx.sym.split(data=source_embed, num_outputs=source_len, squeeze_axis=1)\n",
    "    \n",
    "    target_embed = mx.sym.Embedding(\n",
    "        data=target,\n",
    "        input_dim=target_vocab_size,\n",
    "        output_dim=embed_size,\n",
    "        weight= mx.sym.Variable('target_embed_weight')\n",
    "    )\n",
    "    \n",
    "    #target_vector_of_words = mx.sym.split(data=target_embed, num_outputs=target_len, squeeze_axis=1)\n",
    "\n",
    "    \n",
    "    bi_cell = mx.rnn.BidirectionalCell(\n",
    "        mx.rnn.GRUCell(num_hidden=num_hidden//2, prefix=\"gru1_\"),\n",
    "        mx.rnn.GRUCell(num_hidden=num_hidden//2, prefix=\"gru2_\"),\n",
    "        output_prefix=\"bi_\"\n",
    "    )\n",
    "\n",
    "    encoder = mx.rnn.ResidualCell(bi_cell)\n",
    "\n",
    "    _, encoder_state = encoder.unroll(\n",
    "        length=source_len,\n",
    "        inputs=source_embed,\n",
    "        merge_outputs=False\n",
    "    )\n",
    "\n",
    "    encoder_state = mx.sym.concat(encoder_state[0][0],encoder_state[1][0])\n",
    "\n",
    "    decoder = mx.rnn.GRUCell(num_hidden=num_hidden)\n",
    "\n",
    "    rnn_output, decoder_state = decoder.unroll(\n",
    "        length=target_len,\n",
    "        inputs=target_embed,\n",
    "        begin_state=encoder_state,\n",
    "        merge_outputs=True\n",
    "    )\n",
    "\n",
    "    flat=mx.sym.Flatten(data=rnn_output)\n",
    "\n",
    "    fc=mx.sym.FullyConnected(\n",
    "        data=flat,\n",
    "        num_hidden=target_len*target_vocab_size\n",
    "    )\n",
    "    #drop=mx.sym.Dropout(data=fc, p=0.5)\n",
    "    act=mx.sym.Activation(data=fc, act_type='relu')\n",
    "\n",
    "\n",
    "    out = mx.sym.Reshape(data=act, shape=((0,target_len,target_vocab_size)))\n",
    "\n",
    "    net = mx.sym.SoftmaxOutput(data=out, label=label)\n",
    "\n",
    "    return net, input_names, output_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sym_gen(source_vocab_size, target_vocab_size):\n",
    "    \n",
    "    def _sym_gen(s_t_len):\n",
    "    \n",
    "        return s2s_unroll(\n",
    "            source_len=s_t_len[0],\n",
    "            target_len=s_t_len[1],\n",
    "            input_names=['source','target'],\n",
    "            output_names=['target_softmax_label'],\n",
    "            source_vocab_size=source_vocab_size,\n",
    "            target_vocab_size=target_vocab_size\n",
    "            )\n",
    "    \n",
    "    return _sym_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mx.mod.BucketingModule(\n",
    "        sym_gen=sym_gen(len(source_vocab) + 1, len(target_vocab) + 1),\n",
    "        default_bucket_key=data_train.default_bucket_key,\n",
    "        context=ctx\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_data=data_train,\n",
    "    eval_metric = 'acc',\n",
    "    optimizer=mx.optimizer.Adam(rescale_grad=1/batch_size),\n",
    "    initializer=mx.initializer.Xavier(),\n",
    "    batch_end_callback=mx.callback.Speedometer(batch_size, 10),\n",
    "    num_epoch=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "testset_size=100\n",
    "\n",
    "test_set, inverse_test_set, _, _ = generate_chars_train_eval_sets(dataset_size=testset_size)\n",
    "\n",
    "test_iter = generate_chars_iterators(train_set=test_set, label_set=inverse_test_set, batch_size=1)\n",
    "\n",
    "predictions=model.predict(test_iter)\n",
    "\n",
    "match_count=0\n",
    "for i,pred in enumerate(predictions):\n",
    "    matched = ints2text(onehot2int(mx.ndarray.round(predictions[i]))) == ints2text(inverse_test_set[i])\n",
    "    if matched:\n",
    "        match_count+=1\n",
    "    else:       \n",
    "        print(i)\n",
    "        inverse=ints2text(inverse_test_set[i])\n",
    "        print(inverse)\n",
    "        inverse_pred=ints2text(onehot2int(mx.ndarray.round(predictions[i])))\n",
    "        print(inverse_pred)\n",
    "        print(matched)\n",
    "        for i,s in enumerate(difflib.ndiff(inverse, inverse_pred)):\n",
    "            if s[0]==' ': continue\n",
    "            elif s[0]=='-':\n",
    "                print(u'Delete \"{}\" from position {}'.format(s[-1],i))\n",
    "            elif s[0]=='+':\n",
    "                print(u'Add \"{}\" to position {}'.format(s[-1],i))    \n",
    "        print(\"--------------------\")\n",
    "\n",
    "print(\"Matched %d/%d times\" % (match_count,testset_size))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
