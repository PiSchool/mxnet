{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TACOTRON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import nd, autograd\n",
    "from mxnet import gluon\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx= mx.cpu()\n",
    "num_hidden = 256\n",
    "\n",
    "emb_size=256;\n",
    "\n",
    "data = mx.sym.Variable('data')\n",
    "emb = mx.sym.Embedding(data=data, input_dim=100, output_dim=emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def prenet_pass(data):\n",
    "    prenet = gluon.nn.Sequential() #consider to use HybridSequential\n",
    "    with prenet.name_scope():\n",
    "        prenet.add(gluon.nn.Dense(num_hidden,activation=\"relu\"))\n",
    "        prenet.add(gluon.nn.Dropout(0.5)) #how to use the condition \"if train\" same as TF?\n",
    "        prenet.add(gluon.nn.Dense(num_hidden//2,activation=\"relu\"))\n",
    "        prenet.add(gluon.nn.Dropout(0.5))\n",
    "    prenet.collect_params().initialize(mx.init.Normal(sigma=1.), ctx=ctx)\n",
    "    return prenet(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#banco di filtri convolutivi. Vengono creati K filtri con kernel 1D di dimensione:k \n",
    "def conv1dBank(conv_input, K):\n",
    "    conv=mx.sym.Convolution(data=conv_input, kernel=(1,1), num_filter=emb_size//2)\n",
    "    (conv, mean, var) = mx.sym.BatchNorm(data=conv, output_mean_var=True)\n",
    "    conv = mx.sym.Activation(data=conv, act_type='relu')\n",
    "    for k in range(2, K+1):\n",
    "        convi = mx.sym.Convolution(data=conv_input, kernel=(k,1), num_filter=emb_size//2)\n",
    "        (convi, mean, var) = mx.sym.BatchNorm(data=convi, output_mean_var=True)\n",
    "        convi = mx.sym.Activation(data=convi, act_type='relu')\n",
    "        conv = mx.symbol.concat(conv,convi)\n",
    "    return conv\n",
    "\n",
    "convbank=conv1dBank(emb,16)\n",
    "#mx.viz.plot_network(convbank)\n",
    "#se si usa infer_shape su convbank dando la dimensione dell'input, viene dedotta la shape appunto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#highway\n",
    "def highway_layer(data):\n",
    "    H= mx.symbol.Activation(data=mx.symbol.FullyConnected(data=data, num_hidden=emb_size//2), act_type=\"relu\")\n",
    "    T= mx.symbol.Activation(data=mx.symbol.FullyConnected(data=data, num_hidden=emb_size//2, bias=mx.sym.Variable('bias') ), act_type=\"sigmoid\")\n",
    "    return  H * T + data * (1.0 - T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BidirectionalGRULayer(data):\n",
    "    net = mx.sym.RNN(data=data,bidirectional=True,mode='gru', num_layers=1, state_size=emb_size//2)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#CBHG\n",
    "def CBHG(data):\n",
    "    prenetted_data = prenet_pass(data)\n",
    "    bank = conv1dBank(prenetted_data)\n",
    "    poold_bank = mx.sym.Pooling(data=bank, pool_type='max', kernel=(2, 1), stride=(1,1)) #TOSTUDY: stride?\n",
    "\n",
    "    proj1 = mx.sym.Convolution(data=poold_bank, kernel=(3,1), num_filter=emb_size//2)\n",
    "    (proj1, proj1_mean, proj1_var) = mx.sym.BatchNorm(data=proj1, output_mean_var=True) #TOSTUDY: does the symbol encapsule the mean and var too?\n",
    "    proj1 = mx.sym.Activation(data=proj1, act_type='relu')#Is it ok to declare/use again the same variable?  \n",
    "\n",
    "    proj2 = mx.sym.Convolution(proj1, kernel=(3,1), num_filter=emb_size//2)\n",
    "    (proj2, proj2_mean, proj2_var) = mx.sym.BatchNorm(data=proj2, output_mean_var=True)\n",
    "    residual= proj2 + emb\n",
    "\n",
    "    for i in range(4):\n",
    "        residual = highway(residual)\n",
    "    highway_pass = residual\n",
    "   \n",
    "    encoder = BidirectionalGRULayer(highway_pass)\n",
    "\n",
    "    return encoder\n",
    "\n",
    "#mx.viz.plot_network(CBHG(emb) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2> Attention model part </h2>\n",
    "\n",
    "Need to figure out what is going on here.\n",
    "\n",
    "\"memory\" part = data (embeddings, apples, whatever)-> prenet -> encoder-CBHG   \n",
    "\n",
    "keithito implementation: \n",
    "\n",
    "      # Attention\n",
    "      attention_cell = AttentionWrapper(\n",
    "        DecoderPrenetWrapper(GRUCell(256), is_training),\n",
    "        BahdanauAttention(256, encoder_outputs),\n",
    "        alignment_history=True,\n",
    "        output_attention=False)                                                  # [N, T_in, 256]\n",
    "first arg: a RNN cell \n",
    "second: attention mechanism\n",
    "\n",
    "both AttentionWrapper and BahdanauAttention comes from tf.contrib.seq2seq package\n",
    "    \n",
    "The DecoderPrenetWrapper:\n",
    "<div style=\"background-color:gray\">\n",
    "<code style=\"background-color:gray\">\n",
    "class DecoderPrenetWrapper(RNNCell):\n",
    "  '''Runs RNN inputs through a prenet before sending them to the cell.'''\n",
    "  \n",
    "  bla bla bla\n",
    "  \n",
    "  <b>def call(self, inputs, state):\n",
    "    prenet_out = prenet(inputs, self._is_training, scope='decoder_prenet')\n",
    "    return self._cell(prenet_out, state)\n",
    "  </b>  \n",
    "  bla bla bla\n",
    "</code></div>\n",
    "\n",
    "So it just send data to the prenet \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
