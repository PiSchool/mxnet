{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TACOTRON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import nd, autograd\n",
    "from IPython.display import clear_output\n",
    "ctx= mx.cpu()\n",
    "import csv\n",
    "import codecs\n",
    "import re\n",
    "import audio_process\n",
    "import traceback\n",
    "import subprocess\n",
    "import math\n",
    "from params import Hyperparams as hp \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# command=\"ls train_data/*wav|while read i; do sox $i -n stat 2>&1|grep Leng|cut -d':'  -f 2|sed -r 's/\\s//g'; done|awk 'BEGIN{max_len=0}{if($0>max_len)max_len=$0}END{print max_len}'\"\n",
    "# max_len=subprocess.Popen(command, shell=True, stdout=subprocess.PIPE).stdout.read().strip()\n",
    "# max_audio_sec_len=math.ceil((float(max_len.decode(\"utf-8\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_hidden = 256\n",
    "reduction_factor=2\n",
    "emb_size=256\n",
    "batch_size=1\n",
    "\n",
    "\n",
    "def generate_vocabulary(texts_list):    \n",
    "    # get unique chars and put into a list\n",
    "    return list(set(''.join(texts_list)))\n",
    "    \n",
    "\n",
    "def generate_chars2numbers_mappings(vocabulary):\n",
    "    # create a chars <-> numbers mappings\n",
    "    char2index = {char:i for i,char in enumerate(vocabulary)}\n",
    "    index2char = {i:char for i,char in enumerate(vocabulary)}\n",
    "    \n",
    "    return char2index,index2char\n",
    "\n",
    "\n",
    "def text2numbers(texts_list,char2index_mapping):\n",
    "    numerical_texts=[]\n",
    "    for text in texts_list:\n",
    "        numerical_texts.append([char2index_mapping[char] for char in text])\n",
    "    return numerical_texts\n",
    "\n",
    "def open_data(input_file_path):\n",
    "      \n",
    "    texts, sound_files = [], []\n",
    "    \n",
    "    reader = csv.reader(codecs.open(input_file_path, 'rb', 'utf-8'))\n",
    "    for row in reader:\n",
    "        sound_filename, text = row\n",
    "        sound_file = \"train_data/\" + sound_filename + \".wav\"\n",
    "        text = re.sub(r\"[^ a-z']\", \"\", text.strip().lower())\n",
    "         \n",
    "        texts.append(text)\n",
    "        sound_files.append(sound_file)\n",
    "             \n",
    "    return texts, sound_files\n",
    "# Returns: one-hot-encoded-text, linear spectrum, mel spectrum\n",
    "# Shapes: (data_length, ?, ?), (data_length, (n_fft/2)+1, ceil(max_audio_length))\n",
    "def generate_text_spectra(text_trainset, sound_labels):\n",
    "    \n",
    "    assert len(sound_labels) == len(text_trainset)\n",
    "    \n",
    "    print(\"Generating spectrograms\")\n",
    "    \n",
    "    #tuples of wav and sr of that wav. wav is a 1D floats vector\n",
    "    wavs_srs = [audio_process.load_wave(sound_clip) for sound_clip in sound_labels]\n",
    "    longest_wav_sr = (max(wavs_srs, key= lambda wav: len(wav[0])))\n",
    "    #save the longest audio file length\n",
    "    max_samples_length=(len(longest_wav_sr[0]))\n",
    "    print(\"max audio sample length:\",max_samples_length)\n",
    "\n",
    "    #prepare the data structure for save all the spectra\n",
    "    spectra_lin = mx.ndarray.zeros((len(sound_labels),1+(hp.n_fft//2),math.ceil(max_samples_length/hp.hop_length)))\n",
    "    spectra_mel = mx.ndarray.zeros((len(sound_labels),hp.n_mels,math.ceil(max_samples_length/hp.hop_length)))\n",
    "    print(\"Padding audio and compute mel and lin spectra..\")\n",
    "    for indx,wav_sr in enumerate(wavs_srs):\n",
    "        wav = wav_sr[0]\n",
    "        wav_length = len(wav)\n",
    "#         print(\"wav l\",w_length)\n",
    "        diff = max_samples_length-wav_length\n",
    "#         print(\"num of zeros to add\",diff)\n",
    "        padded = np.append(wav,np.zeros(diff))\n",
    "        # get the spectrum from the padded sound\n",
    "        spectrum_lin, spectrum_mel=(audio_process.do_spectrogram(y=padded,sr=hp.sr))\n",
    "#         print(padded_spectrum_lin.shape)\n",
    "        # save into the ndarray\n",
    "        spectra_lin[indx,:,:]=spectrum_lin[:,:]\n",
    "        spectra_mel[indx,:,:]=spectrum_mel[:,:]\n",
    "    \n",
    "    \n",
    "    print(\"Processing text..\")\n",
    "    vocabulary = generate_vocabulary(texts_list)\n",
    "    vocab_size=len(vocabulary)\n",
    "    char2index,index2char = generate_chars2numbers_mappings(vocabulary)\n",
    "\n",
    "    print(\"Converting text to integers..\")\n",
    "    texts_numerical = text2numbers(texts_list,char2index)\n",
    "    # simulate a different sequence length\n",
    "#   /D E L E T E M E/ \n",
    "    texts_numerical[4]=np.concatenate((texts_numerical[4],[8,9]))\n",
    "#   /D E L E T E M E/\n",
    "\n",
    "    longest_sequence = (max(texts_numerical, key= lambda seq: len(seq)))\n",
    "    longest_sequence_len=len(longest_sequence)\n",
    "    print(\"Pad sequences to\",longest_sequence_len,\"..\")\n",
    "    # helper function for the lambda expression\n",
    "    def _padseq(seq,max_len):\n",
    "        diff=max_len-len(seq)\n",
    "        if diff>0: \n",
    "            # SHITTY USELESS MXNET API. CANNOT CONCAT A NON-EMPTY WITH EMPTY ARRAY. \n",
    "            # EDIT: use numpy now. Still using this condition for safety\n",
    "            pad = np.zeros(diff)-1\n",
    "            seq=np.append(seq,[pad])\n",
    "        return seq\n",
    "    \n",
    "    padded_sequences = mx.nd.array(\n",
    "        list(\n",
    "            map(\n",
    "                lambda seq: _padseq(seq,longest_sequence_len), texts_numerical\n",
    "            )\n",
    "        )\n",
    "    )   \n",
    "    \n",
    "    texts_one_hot=mx.ndarray.one_hot(padded_sequences,vocab_size)\n",
    "    \n",
    "    return texts_one_hot, spectra_lin, spectra_mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating spectrograms\n",
      "max audio sample length: 17567\n",
      "Padding and compute spectrum\n",
      "Processing text..\n",
      "Converting text to integers..\n",
      "Get the length of the longest sequence..\n",
      "Pad sequences..\n",
      "I will take those for eval: [34 28 44 29  9]\n",
      "..and the remaining for train: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 30 31 32 33 35 36 37 38 39 40 41 42 43 45 46 47 48 49] \n",
      "\n",
      "train data shape: (45, 80, 176) train label shape: (45, 1025, 176)\n",
      "eval data shape: (5, 80, 176) eval label shape: (5, 1025, 176) \n",
      "\n",
      "Populating traindata iterator\n",
      "Populating evaldata iterator\n",
      "(<mxnet.io.NDArrayIter object at 0x7fc7d410a198>, <mxnet.io.NDArrayIter object at 0x7fc7d410a240>)\n"
     ]
    }
   ],
   "source": [
    "def get_iterators(data='train_data/dataset.csv'):\n",
    "    texts_list, sound_files_list = open_data(data)\n",
    "    size=len(sound_files_list)\n",
    "\n",
    "    texts_one_hot, spectra_lin, spectra_mel = generate_train_eval_data(texts_list, sound_files_list)\n",
    "\n",
    "    # get 10% of dataset as eval data \n",
    "    eval_indxs = (np.random.randint(0, high=size, size=size//10))\n",
    "    # remaining indexes for the train\n",
    "    train_indxs = np.setdiff1d(np.arange(size),eval_indxs)\n",
    "\n",
    "    print(\"I will take those for eval:\",eval_indxs)\n",
    "    print(\"..and the remaining for train:\",train_indxs,\"\\n\")\n",
    "\n",
    "    #take from the array (1st arg) the indexes of the first dimension specified by the 2nd arg\n",
    "    #train_txt take the one_hot matrices\n",
    "    train_txt_data = mx.ndarray.take(texts_one_hot,mx.nd.array(train_indxs))\n",
    "    eval_txt_data = mx.ndarray.take(texts_one_hot,mx.nd.array(eval_indxs))\n",
    "\n",
    "    train_data = mx.ndarray.take(spectra_mel,mx.nd.array(train_indxs))\n",
    "    train_label = mx.ndarray.take(spectra_lin,mx.nd.array(train_indxs))\n",
    "\n",
    "    eval_data = mx.ndarray.take(spectra_mel,mx.nd.array(eval_indxs))\n",
    "    eval_label = mx.ndarray.take(spectra_lin,mx.nd.array(eval_indxs))\n",
    "\n",
    "    print(\"train data shape:\",train_data.shape,\"train label shape:\",train_label.shape)\n",
    "    print(\"eval data shape:\", eval_data.shape,\"eval label shape:\",eval_label.shape,\"\\n\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        print(\"Populating traindata iterator\")\n",
    "        traindata_iterator = mx.io.NDArrayIter(data={'mel_spectrogram':train_data},\n",
    "                                label={'linear_spectrogram':train_label},\n",
    "                                batch_size=batch_size)\n",
    "        print(\"Populating evaldata iterator\")\n",
    "        evaldata_iterator = mx.io.NDArrayIter(data={'mel_spectrogram':eval_data},\n",
    "                                label={'linear_spectrogram':eval_label},\n",
    "                                batch_size=batch_size)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "#     for batch in traindata_iterator:\n",
    "#         print(batch.data[0].asnumpy())\n",
    "#         print(batch.data[0].shape)\n",
    "    \n",
    "    return traindata_iterator,evaldata_iterator\n",
    "\n",
    "print(get_iterators())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FC-256-ReLU → Dropout(0.5) → FC-128-ReLU → Dropout(0.5)\n",
    "\"\"\"\n",
    "def prenet_pass(data):\n",
    "    fc1 = mx.symbol.FullyConnected(data=data, num_hidden=emb_size, name='prenet_fc1')\n",
    "    act1 = mx.symbol.Activation(data=fc1, act_type='relu', name='prenet_act1')\n",
    "    drop1 = mx.symbol.Dropout(act1, p=0.5, name='prenet_drop1')\n",
    "    \n",
    "    fc2 = mx.symbol.FullyConnected(data=drop1, num_hidden=emb_size//2, name='prenet_fc2')\n",
    "    act2 = mx.symbol.Activation(data=fc2, act_type='relu', name='prenet_act2')\n",
    "    prenet_output = mx.symbol.Dropout(act2, p=0.5, name='prenet_drop2')\n",
    "    \n",
    "    return prenet_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# banco di filtri convolutivi. Vengono creati K filtri con kernel 1D di dimensione:k \n",
    "def conv1dBank(conv_input, K):\n",
    "    conv=mx.sym.Convolution(data=conv_input, kernel=(1,1), num_filter=emb_size//2)\n",
    "    (conv, mean, var) = mx.sym.BatchNorm(data=conv, output_mean_var=True)\n",
    "    conv = mx.sym.Activation(data=conv, act_type='relu')\n",
    "    for k in range(2, K+1):\n",
    "        convi = mx.sym.Convolution(data=conv_input, kernel=(k,1), num_filter=emb_size//2)\n",
    "        (convi, mean, var) = mx.sym.BatchNorm(data=convi, output_mean_var=True)\n",
    "        convi = mx.sym.Activation(data=convi, act_type='relu')\n",
    "        conv = mx.symbol.concat(conv,convi)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# highway\n",
    "def highway_layer(data):\n",
    "    H= mx.symbol.Activation(\n",
    "        data=mx.symbol.FullyConnected(data=data, num_hidden=emb_size//2, name=\"highway_fcH\"),\n",
    "        act_type=\"relu\"\n",
    "    )\n",
    "    T= mx.symbol.Activation(\n",
    "        data=mx.symbol.FullyConnected(data=data, num_hidden=emb_size//2, bias=mx.sym.Variable('bias'), name=\"highway_fcT\"),\n",
    "        act_type=\"sigmoid\"\n",
    "    )\n",
    "    return  H * T + data * (1.0 - T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CBHG\n",
    "def CBHG(data,K,proj1_size,proj2_size):\n",
    "    #se si usa infer_shape su convbank dando la dimensione dell'input, viene dedotta la shape appunto \n",
    "    bank = conv1dBank(data,K)\n",
    "    poold_bank = mx.sym.Pooling(data=bank, pool_type='max', kernel=(2, 1), stride=(1,1), name=\"CBHG_pool\")\n",
    "\n",
    "    proj1 = mx.sym.Convolution(data=poold_bank, kernel=(3,1), num_filter=proj1_size, name='CBHG_conv1')\n",
    "    (proj1, proj1_mean, proj1_var) = mx.sym.BatchNorm(data=proj1, output_mean_var=True, name='CBHG_batch1')\n",
    "    proj1 = mx.sym.Activation(data=proj1, act_type='relu', name='CBHG_act1')\n",
    "\n",
    "    proj2 = mx.sym.Convolution(proj1, kernel=(3,1), num_filter=proj2_size, name='CBHG_conv2')\n",
    "    (proj2, proj2_mean, proj2_var) = mx.sym.BatchNorm(data=proj2, output_mean_var=True, name='CBHG_batch2')\n",
    "    \n",
    "    residual= proj2 + data\n",
    "\n",
    "    for i in range(4):\n",
    "        residual = highway_layer(residual)\n",
    "    highway_pass = residual\n",
    "   \n",
    "    bidirectional_gru_cell = mx.rnn.BidirectionalCell(\n",
    "        mx.rnn.GRUCell(num_hidden=emb_size//2, prefix='CBHG_gru1'),\n",
    "        mx.rnn.GRUCell(num_hidden=emb_size//2, prefix='CBHG_gru2'),\n",
    "        output_prefix='CBHG_bi_'\n",
    "    )\n",
    "    outputs, states = bidirectional_gru_cell.unroll(1, inputs=highway_pass, merge_outputs=True)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoder\n",
    "def encoder(data):\n",
    "    embed_vector = mx.sym.Embedding(data=data, input_dim=longest_word, output_dim=emb_size, name='encoder_embed')\n",
    "    prenet_output = prenet_pass(embed_vector)\n",
    "    return CBHG(prenet_output,16, emb_size//2, emb_size//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = mx.sym.Variable('text')\n",
    "\n",
    "encoded = encoder(text)\n",
    "graph=mx.viz.plot_network(\n",
    "    encoded,\n",
    "    save_format='pdf',\n",
    "    title='encoder')\n",
    "#graph.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# decoder\n",
    "def decoder(input_spectrogram,context,reduction_factor):\n",
    "    #embed_vector = mx.sym.Embedding(data=input_spectrogram, input_dim=80, output_dim=emb_size, name='decoder_embed')\n",
    "    prenet_output = prenet_pass(input_spectrogram)\n",
    "        \n",
    "    stack = mx.rnn.SequentialRNNCell()\n",
    "    stack.add(mx.rnn.GRUCell(num_hidden=emb_size,prefix='decoder_layer1_'))\n",
    "    stack.add(mx.rnn.GRUCell(num_hidden=emb_size,prefix='decoder_layer2_'))\n",
    "    \n",
    "    residual_gru_stack = mx.rnn.ResidualCell(stack)\n",
    "    \n",
    "    gru_outputs,states = residual_gru_stack.unroll(length=1,\n",
    "                                               inputs=prenet_output,\n",
    "                                               begin_state=context,\n",
    "                                               merge_outputs=True)\n",
    "\n",
    "    predicted_frames = mx.symbol.Activation(\n",
    "        data=mx.symbol.FullyConnected(data=gru_outputs, num_hidden=80*reduction_factor),\n",
    "        act_type=\"relu\"\n",
    "    )\n",
    "    \n",
    "    return predicted_frames, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def postprocess(input_mel_spectgrograms):\n",
    "    linear_scale_spectrograms=CBHG(input_mel_spectgrograms,8,emb_size,80)\n",
    "    return linear_scale_spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_spectrogram = mx.sym.Variable('linear_spectrogram')\n",
    "\n",
    "spectrograms_count=5 #dummy value\n",
    "decoder_state=[encoded,encoded]\n",
    "predicted_frames=mx.sym.zeros((1,80))\n",
    "full_frame=mx.sym.zeros((1,80))\n",
    "mel_spectrogram = mx.sym.Variable('mel_spectrogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = mx.sym.MAERegressionOutput(data=postprocess(mel_spectrogram), label=linear_spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = mx.mod.Module(symbol=net,\n",
    "                      context=ctx,\n",
    "                      data_names=['mel_spectrogram'],\n",
    "                      label_names=['linear_spectrogram']\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data provided by data_shapes don't match names specified by data_names ([DataDesc[_0_data,(2, 4),<class 'numpy.float32'>,NCHW], DataDesc[_1_data,(2, 4),<class 'numpy.float32'>,NCHW], DataDesc[_2_data,(2, 4),<class 'numpy.float32'>,NCHW], DataDesc[_3_data,(2, 4),<class 'numpy.float32'>,NCHW], DataDesc[_4_data,(2, 4),<class 'numpy.float32'>,NCHW], DataDesc[_5_data,(2, 4),<class 'numpy.float32'>,NCHW], DataDesc[_6_data,(2, 4),<class 'numpy.float32'>,NCHW], DataDesc[_7_data,(2, 4),<class 'numpy.float32'>,NCHW], DataDesc[_8_data,(2, 4),<class 'numpy.float32'>,NCHW], DataDesc[_9_data,(2, 4),<class 'numpy.float32'>,NCHW]] vs. ['mel_spectrogram'])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-c80217d66369>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0moptimizer_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'momentum'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           num_epoch=8)\n\u001b[0m",
      "\u001b[0;32m~/pischool/mxnet/venv3/local/lib/python3.5/site-packages/mxnet/module/base_module.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, eval_data, eval_metric, epoch_end_callback, batch_end_callback, kvstore, optimizer, optimizer_params, eval_end_callback, eval_batch_end_callback, initializer, arg_params, aux_params, allow_missing, force_rebind, force_init, begin_epoch, num_epoch, validation_metric, monitor)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         self.bind(data_shapes=train_data.provide_data, label_shapes=train_data.provide_label,\n\u001b[0;32m--> 460\u001b[0;31m                   for_training=True, force_rebind=force_rebind)\n\u001b[0m\u001b[1;32m    461\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmonitor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstall_monitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pischool/mxnet/venv3/local/lib/python3.5/site-packages/mxnet/module/module.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, data_shapes, label_shapes, for_training, inputs_need_grad, force_rebind, shared_module, grad_req)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         self._data_shapes, self._label_shapes = _parse_data_desc(\n\u001b[0;32m--> 400\u001b[0;31m             self.data_names, self.label_names, data_shapes, label_shapes)\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshared_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pischool/mxnet/venv3/local/lib/python3.5/site-packages/mxnet/module/base_module.py\u001b[0m in \u001b[0;36m_parse_data_desc\u001b[0;34m(data_names, label_names, data_shapes, label_shapes)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;34m\"\"\"parse data_attrs into DataDesc format and check that names match\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mdata_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataDesc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDataDesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_shapes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0m_check_names_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabel_shapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mlabel_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataDesc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDataDesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_shapes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pischool/mxnet/venv3/local/lib/python3.5/site-packages/mxnet/module/base_module.py\u001b[0m in \u001b[0;36m_check_names_match\u001b[0;34m(data_names, data_shapes, name, throw)\u001b[0m\n\u001b[1;32m     61\u001b[0m             name, name, str(data_shapes), str(data_names))\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthrow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data provided by data_shapes don't match names specified by data_names ([DataDesc[_0_data,(2, 4),<class 'numpy.float32'>,NCHW], DataDesc[_1_data,(2, 4),<class 'numpy.float32'>,NCHW], DataDesc[_2_data,(2, 4),<class 'numpy.float32'>,NCHW], DataDesc[_3_data,(2, 4),<class 'numpy.float32'>,NCHW], DataDesc[_4_data,(2, 4),<class 'numpy.float32'>,NCHW], DataDesc[_5_data,(2, 4),<class 'numpy.float32'>,NCHW], DataDesc[_6_data,(2, 4),<class 'numpy.float32'>,NCHW], DataDesc[_7_data,(2, 4),<class 'numpy.float32'>,NCHW], DataDesc[_8_data,(2, 4),<class 'numpy.float32'>,NCHW], DataDesc[_9_data,(2, 4),<class 'numpy.float32'>,NCHW]] vs. ['mel_spectrogram'])"
     ]
    }
   ],
   "source": [
    "model.fit(traindata_iterator,\n",
    "          eval_data=evaldata_iterator,\n",
    "          optimizer=mx.optimizer.Adam,\n",
    "          optimizer_params={'learning_rate': 0.1, 'momentum': 0.9},\n",
    "          eval_metric='acc',\n",
    "          num_epoch=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(spectrograms_count):\n",
    "    predicted_frames,decoder_state = decoder(predicted_frames,decoder_state,reduction_factor)\n",
    "    full_frame=mx.sym.concat(full_frame,predicted_frames)\n",
    "\n",
    "spectral_magnitude=CBHG(full_frame, 8, emb_size, 80)\n",
    "\n",
    "graph=mx.viz.plot_network(\n",
    "    spectral_magnitude,\n",
    "    save_format='pdf',\n",
    "    title='decoder')\n",
    "#graph.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
