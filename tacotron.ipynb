{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>TACOTRON</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import nd, autograd\n",
    "from IPython.display import clear_output\n",
    "ctx= mx.cpu()\n",
    "import csv\n",
    "import codecs\n",
    "import re\n",
    "import audio_process\n",
    "import traceback\n",
    "import subprocess\n",
    "import math\n",
    "from params import Hyperparams as hp \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> DATA SETUP </h3>\n",
    "\n",
    "<b>Data</b>: \n",
    "text - mel spectrograms - linear spectrograms\n",
    "\n",
    "<b>Shapes</b>: \n",
    "(batch_size, pad_to_max_text_length) - (batch_size, 80, pad_to_max_audio_length) - (batch_size, 1025, pad_to_max_audio_length) \n",
    "\n",
    "<b>Note_1</b>: I'm using a little batch size due my little dummy train dataset <br/>\n",
    "<b>Note_2</b>: on Tensorflow implementation there is a reshape step by reduction factor r described in the paper.<br/> Tensorflow data got these shapes:\n",
    "\n",
    "<b>text</b>: \n",
    "(batch_size,length_text)\n",
    "<b>mel spectrograms</b>:\n",
    "(batch_size, time_frames, 80&ast;r)\n",
    "<b>linear spectrograms</b>:\n",
    "(batch_size, time_frames, 1025&ast;r)\n",
    "\n",
    "<br/>\n",
    "more info at: https://github.com/Kyubyong/tacotron/blob/master/utils.py#L58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_hidden = hp.embed_size\n",
    "reduction_factor=hp.r\n",
    "emb_size=hp.embed_size\n",
    "batch_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_vocabulary(texts_list):    \n",
    "    # get unique chars and put into a list\n",
    "    return list(set(''.join(texts_list)))\n",
    "    \n",
    "\n",
    "def generate_chars2numbers_mappings(vocabulary):\n",
    "    # create a chars <-> numbers mappings\n",
    "    char2index = {char:i for i,char in enumerate(vocabulary)}\n",
    "    index2char = {i:char for i,char in enumerate(vocabulary)}\n",
    "    \n",
    "    return char2index,index2char\n",
    "\n",
    "\n",
    "def text2numbers(texts_list,char2index_mapping):\n",
    "    numerical_texts=[]\n",
    "    for text in texts_list:\n",
    "        numerical_texts.append([char2index_mapping[char] for char in text])\n",
    "    return numerical_texts\n",
    "\n",
    "def open_data(input_file_path):\n",
    "      \n",
    "    texts, sound_files = [], []\n",
    "    \n",
    "    reader = csv.reader(codecs.open(input_file_path, 'rb', 'utf-8'))\n",
    "    for row in reader:\n",
    "        sound_filename, text = row\n",
    "        sound_file = hp.sound_fpath +\"/\"+ sound_filename + \".wav\"\n",
    "        text = re.sub(r\"[^ a-z']\", \"\", text.strip().lower())\n",
    "         \n",
    "        texts.append(text)\n",
    "        sound_files.append(sound_file)\n",
    "             \n",
    "    return texts, sound_files\n",
    "# Returns: one-hot-encoded-text, linear spectrum, mel spectrum\n",
    "# Shapes: (data_length, ?, ?) , (data_length, (n_fft/2)+1, ceil(max_audio_length/hop_size)), (data_length, n_mels, ceil(max_audio_length/hop_size))\n",
    "def generate_text_spectra(texts_list, sound_labels):\n",
    "    \n",
    "    assert len(sound_labels) == len(texts_list)\n",
    "    \n",
    "    print(\"Generating spectrograms\")\n",
    "    \n",
    "    #tuples of wav and sr of that wav. wav is a 1D floats vector\n",
    "    wavs_srs = [audio_process.load_wave(sound_clip) for sound_clip in sound_labels]\n",
    "    longest_wav_sr = (max(wavs_srs, key= lambda wav: len(wav[0])))\n",
    "    #save the longest audio file length\n",
    "    max_samples_length=(len(longest_wav_sr[0]))\n",
    "    print(\"max audio sample length:\",max_samples_length)\n",
    "\n",
    "    #prepare the data structure for save all the spectra\n",
    "    spectra_lin = mx.ndarray.zeros((len(sound_labels),math.ceil(max_samples_length/hp.hop_length),1+(hp.n_fft//2)))\n",
    "    spectra_mel = mx.ndarray.zeros((len(sound_labels),math.ceil(max_samples_length/hp.hop_length),hp.n_mels))\n",
    "    print(\"Padding audio and compute mel and lin spectra..\")\n",
    "    for indx,wav_sr in enumerate(wavs_srs):\n",
    "        wav = wav_sr[0]\n",
    "        wav_length = len(wav)\n",
    "#         print(\"wav l\",w_length)\n",
    "        diff = max_samples_length-wav_length\n",
    "#         print(\"num of zeros to add\",diff)\n",
    "        padded = np.append(wav,np.zeros(diff))\n",
    "        # get the spectrum from the padded sound\n",
    "        spectrum_lin, spectrum_mel=(audio_process.do_spectrogram(y=padded,sr=hp.sr))\n",
    "#         print(padded_spectrum_lin.shape)\n",
    "        # save into the ndarray\n",
    "        spectra_lin[indx,:,:]=np.transpose(spectrum_lin[:,:])\n",
    "        spectra_mel[indx,:,:]=np.transpose(spectrum_mel[:,:])\n",
    "    \n",
    "    \n",
    "    print(\"Processing text..\")\n",
    "    vocabulary = generate_vocabulary(texts_list)\n",
    "    vocab_size=len(vocabulary)\n",
    "    char2index,index2char = generate_chars2numbers_mappings(vocabulary)\n",
    "\n",
    "    print(\"Converting text to integers..\")\n",
    "    texts_numerical = text2numbers(texts_list,char2index)\n",
    "    # simulate a different sequence length\n",
    "#   /D E L E T E M E/ \n",
    "    texts_numerical[4]=np.concatenate((texts_numerical[4],[8,9]))\n",
    "#   /D E L E T E M E/\n",
    "\n",
    "    longest_sequence = (max(texts_numerical, key= lambda seq: len(seq)))\n",
    "    longest_sequence_len=len(longest_sequence)\n",
    "    print(\"Pad sequences to\",longest_sequence_len,\"..\")\n",
    "    # helper function for the lambda expression\n",
    "    def _padseq(seq,max_len):\n",
    "        diff=max_len-len(seq)\n",
    "        if diff>0: \n",
    "            # SHITTY USELESS MXNET API. CANNOT CONCAT A NON-EMPTY WITH EMPTY ARRAY. \n",
    "            # EDIT: use numpy now. Still using this condition for safety\n",
    "            pad = np.zeros(diff)-1\n",
    "            seq=np.append(seq,[pad])\n",
    "        return seq\n",
    "    \n",
    "    padded_sequences = mx.nd.array(\n",
    "        list(\n",
    "            map(\n",
    "                lambda seq: _padseq(seq,longest_sequence_len), texts_numerical\n",
    "            )\n",
    "        )\n",
    "    )   \n",
    "    \n",
    "    texts_one_hot=mx.ndarray.one_hot(padded_sequences,vocab_size)\n",
    "    \n",
    "    return texts_one_hot, spectra_lin, spectra_mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_iterators(data='../train_data/dataset.csv'):\n",
    "    texts_list, sound_files_list = open_data(data)\n",
    "    size=len(sound_files_list)\n",
    "\n",
    "    texts_one_hot, spectra_lin, spectra_mel = generate_text_spectra(texts_list, sound_files_list)\n",
    "\n",
    "    # get 10% of dataset as eval data \n",
    "    eval_indxs = (np.random.randint(0, high=size, size=size//10))\n",
    "    # remaining indexes for the train\n",
    "    train_indxs = np.setdiff1d(np.arange(size),eval_indxs)\n",
    "\n",
    "    print(\"I will take those for eval:\",eval_indxs)\n",
    "    print(\"..and the remaining for train:\",train_indxs,\"\\n\")\n",
    "\n",
    "    #take from the array (1st arg) the indexes of the first dimension specified by the 2nd arg\n",
    "    #train_txt take the one_hot matrices\n",
    "    train_txt_data = mx.ndarray.take(texts_one_hot,mx.nd.array(train_indxs))\n",
    "    eval_txt_data = mx.ndarray.take(texts_one_hot,mx.nd.array(eval_indxs))\n",
    "\n",
    "    train_data = mx.ndarray.take(spectra_mel,mx.nd.array(train_indxs))\n",
    "    train_label = mx.ndarray.take(spectra_lin,mx.nd.array(train_indxs))\n",
    "\n",
    "    eval_data = mx.ndarray.take(spectra_mel,mx.nd.array(eval_indxs))\n",
    "    eval_label = mx.ndarray.take(spectra_lin,mx.nd.array(eval_indxs))\n",
    "\n",
    "    print(\"train data shape:\",train_data.shape,\"train label shape:\",train_label.shape)\n",
    "    print(\"eval data shape:\", eval_data.shape,\"eval label shape:\",eval_label.shape,\"\\n\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        print(\"Populating traindata iterator\")\n",
    "        traindata_iterator = mx.io.NDArrayIter(data={'mel_spectrogram':train_data},\n",
    "                                label={'linear_spectrogram':train_label},\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True)\n",
    "        print(\"Populating evaldata iterator\")\n",
    "        evaldata_iterator = mx.io.NDArrayIter(data={'mel_spectrogram':eval_data},\n",
    "                                label={'linear_spectrogram':eval_label},\n",
    "                                batch_size=batch_size)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "#     for batch in traindata_iterator:\n",
    "#         print(batch.data[0].asnumpy())\n",
    "#         print(batch.data[0].shape)\n",
    "    \n",
    "    return traindata_iterator,evaldata_iterator, train_data.shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Modules </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Prenet </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FC-256-ReLU → Dropout(0.5) → FC-128-ReLU → Dropout(0.5)\n",
    "\"\"\"\n",
    "def prenet_pass(data):\n",
    "    fc1 = mx.symbol.FullyConnected(data=data, num_hidden=emb_size, name='prenet_fc1',flatten=False)\n",
    "    act1 = mx.symbol.Activation(data=fc1, act_type='relu', name='prenet_act1')\n",
    "    drop1 = mx.symbol.Dropout(act1, p=0.5, name='prenet_drop1')\n",
    "    \n",
    "    fc2 = mx.symbol.FullyConnected(data=drop1, num_hidden=emb_size//2, name='prenet_fc2', flatten=False)\n",
    "    act2 = mx.symbol.Activation(data=fc2, act_type='relu', name='prenet_act2')\n",
    "    prenet_output = mx.symbol.Dropout(act2, p=0.5, name='prenet_drop2')\n",
    "    \n",
    "    return prenet_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Convolution 1D Bank </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolution bank of K filter\n",
    "def conv1dBank(conv_input, K):\n",
    "    #The k-th filter got a kernel width of k, with 0<k<=K \n",
    "    conv=mx.sym.Convolution(data=conv_input, kernel=(1,), num_filter=hp.embed_size//2,name=\"convBank_1\",layout='NCW')\n",
    "    (conv, mean, var) = mx.sym.BatchNorm(data=conv, output_mean_var=True)\n",
    "    conv = mx.sym.Activation(data=conv, act_type='relu')\n",
    "\n",
    "    for k in range(2, K+1):\n",
    "        in_i = mx.sym.concat(conv_input,mx.sym.zeros((batch_size,hp.embed_size//2,k-1)),dim=2)\n",
    "        convi = mx.sym.Convolution(data=in_i, kernel=(k,), num_filter=hp.embed_size//2,layout='NCW',name=\"convBank_\"+str(k))\n",
    "        (convi, mean, var) = mx.sym.BatchNorm(data=convi, output_mean_var=True)\n",
    "        convi = mx.sym.Activation(data=convi, act_type='relu')\n",
    "        conv = mx.symbol.concat(conv,convi,dim=1)    \n",
    "    return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Highway </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# highway\n",
    "def highway_layer(data,i=0):\n",
    "    H= mx.symbol.Activation(\n",
    "        data=mx.symbol.FullyConnected(data=data, num_hidden=emb_size//2, name=\"highway_fcH_\"+str(i),flatten=False),\n",
    "        act_type=\"relu\"\n",
    "    )\n",
    "    T= mx.symbol.Activation(\n",
    "        data=mx.symbol.FullyConnected(data=data, num_hidden=emb_size//2, bias=mx.sym.Variable('bias'+str(i),init=mx.initializer.Normal()), name=\"highway_fcT\"+str(i),flatten=False),\n",
    "        act_type=\"sigmoid\"\n",
    "    )\n",
    "    return  H * T + data * (1.0 - T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> CBHG </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CBHG\n",
    "def CBHG(data,K,proj1_size,proj2_size,num_unroll):\n",
    "    bank = conv1dBank(data,K)\n",
    "\n",
    "    #After the convolutional bank, a max pooling is applied\n",
    "    #Again here. To obtain always the same dimension I'm padding the input of each operation\n",
    "    conv_padded = mx.sym.concat(bank,mx.sym.zeros((batch_size,K*(hp.embed_size//2),1)),dim=2)\n",
    "    poold_bank = mx.sym.Pooling(data=conv_padded, pool_type='max', kernel=(2,), stride=(1,), name=\"CBHG_pool\")\n",
    "\n",
    "    #Now two other projections (convolutions) are done. Same padding thing\n",
    "    poold_bank_padded = mx.sym.concat(poold_bank,mx.sym.zeros((batch_size,K*(hp.embed_size//2),2)),dim=2)\n",
    "    proj1 = mx.sym.Convolution(data=poold_bank_padded, kernel=(3,), num_filter=proj1_size, name='CBHG_conv1',layout='NCW')\n",
    "    (proj1, proj1_mean, proj1_var) = mx.sym.BatchNorm(data=proj1, output_mean_var=True, name='CBHG_batch1')\n",
    "    proj1 = mx.sym.Activation(data=proj1, act_type='relu', name='CBHG_act1')\n",
    "\n",
    "    proj1_padded = mx.sym.concat(proj1,mx.sym.zeros((batch_size,hp.embed_size,2)),dim=2)\n",
    "    proj2 = mx.sym.Convolution(proj1_padded, kernel=(3,), num_filter=proj2_size, name='CBHG_conv2',layout='NCW')\n",
    "    (proj2, proj2_mean, proj2_var) = mx.sym.BatchNorm(data=proj2, output_mean_var=True, name='CBHG_batch2')\n",
    "\n",
    "    #Adding residual connection. The output of the prenet pass is added to proj2\n",
    "    residual= proj2 + data\n",
    "\n",
    "    residual = mx.sym.swapaxes(residual,1,2)\n",
    "    \n",
    "    #A 4 highway layers is created\n",
    "    for i in range(4):\n",
    "        residual = highway_layer(residual,i)\n",
    "    highway_pass = residual\n",
    "\n",
    "    #The highway output is passed to the bidirectional gru cell\n",
    "    bidirectional_gru_cell = mx.rnn.BidirectionalCell(\n",
    "        mx.rnn.GRUCell(num_hidden=hp.embed_size//2, prefix='CBHG_gru1'),\n",
    "        mx.rnn.GRUCell(num_hidden=hp.embed_size//2, prefix='CBHG_gru2'),\n",
    "        output_prefix='CBHG_bi_'\n",
    "    )\n",
    "\n",
    "    bi_gru_outputs, bi_gru_states = bidirectional_gru_cell.unroll(num_unroll, inputs=highway_pass, merge_outputs=True)\n",
    "    \n",
    "    return bi_gru_outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Encoder </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoder\n",
    "def encoder(data):\n",
    "    embed_vector = mx.sym.Embedding(data=data, input_dim=longest_word, output_dim=emb_size, name='encoder_embed')\n",
    "    prenet_output = prenet_pass(embed_vector)\n",
    "    return CBHG(prenet_output,16, emb_size//2, emb_size//2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Decoder (stub)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# decoder\n",
    "def decoder(input_spectrogram,context,reduction_factor):\n",
    "    #embed_vector = mx.sym.Embedding(data=input_spectrogram, input_dim=80, output_dim=emb_size, name='decoder_embed')\n",
    "    prenet_output = prenet_pass(input_spectrogram)\n",
    "        \n",
    "    stack = mx.rnn.SequentialRNNCell()\n",
    "    stack.add(mx.rnn.GRUCell(num_hidden=emb_size,prefix='decoder_layer1_'))\n",
    "    stack.add(mx.rnn.GRUCell(num_hidden=emb_size,prefix='decoder_layer2_'))\n",
    "    \n",
    "    residual_gru_stack = mx.rnn.ResidualCell(stack)\n",
    "    \n",
    "    gru_outputs,states = residual_gru_stack.unroll(length=1,\n",
    "                                               inputs=prenet_output,\n",
    "                                               begin_state=context,\n",
    "                                               merge_outputs=True)\n",
    "\n",
    "    predicted_frames = mx.symbol.Activation(\n",
    "        data=mx.symbol.FullyConnected(data=gru_outputs, num_hidden=80*reduction_factor),\n",
    "        act_type=\"relu\"\n",
    "    )\n",
    "    \n",
    "    return predicted_frames, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def postprocess(input_mel_spectgrograms,max_audio_length):\n",
    "    in_cbhg = prenet_pass(input_mel_spectgrograms)\n",
    "    in_cbhg_sw= mx.sym.swapaxes(in_cbhg,1,2)\n",
    "    \n",
    "    bi_gru_out =CBHG(in_cbhg_sw,8,hp.embed_size,hp.embed_size//2,max_audio_length)\n",
    "    \n",
    "    linear_scale_spectrograms = mx.symbol.FullyConnected(data=bi_gru_out,num_hidden=(hp.n_fft//2)+1,flatten=False)\n",
    "    return linear_scale_spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating spectrograms\n",
      "max audio sample length: 17567\n",
      "Padding audio and compute mel and lin spectra..\n",
      "Processing text..\n",
      "Converting text to integers..\n",
      "Pad sequences to 7 ..\n",
      "I will take those for eval: [12  6 49  1  9]\n",
      "..and the remaining for train: [ 0  2  3  4  5  7  8 10 11 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48] \n",
      "\n",
      "train data shape: (45, 176, 80) train label shape: (45, 176, 1025)\n",
      "eval data shape: (5, 176, 80) eval label shape: (5, 176, 1025) \n",
      "\n",
      "Populating traindata iterator\n",
      "Populating evaldata iterator\n",
      "max_audio_length:  176\n"
     ]
    }
   ],
   "source": [
    "traindata_iterator, evaldata_iterator, max_audio_length = get_iterators(data=hp.text_file)\n",
    "linear_spectrogram = mx.sym.Variable('linear_spectrogram')\n",
    "mel_spectrogram = mx.sym.Variable('mel_spectrogram')\n",
    "print(\"max_audio_length: \",max_audio_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "net = mx.sym.MAERegressionOutput(data=postprocess(mel_spectrogram,max_audio_length), label=linear_spectrogram)\n",
    "model = mx.mod.Module(symbol=net,\n",
    "                      context=ctx,\n",
    "                      data_names=['mel_spectrogram'],\n",
    "                      label_names=['linear_spectrogram']\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9e3696d833da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mbatch_end_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpeedometer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m           num_epoch=8)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mxnet/lib/python3.6/site-packages/mxnet/module/base_module.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, eval_data, eval_metric, epoch_end_callback, batch_end_callback, kvstore, optimizer, optimizer_params, eval_end_callback, eval_batch_end_callback, initializer, arg_params, aux_params, allow_missing, force_rebind, force_init, begin_epoch, num_epoch, validation_metric, monitor)\u001b[0m\n\u001b[1;32m    494\u001b[0m                     \u001b[0mend_of_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmonitor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet/lib/python3.6/site-packages/mxnet/module/module.py\u001b[0m in \u001b[0;36mupdate_metric\u001b[0;34m(self, eval_metric, labels)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0mTypically\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \"\"\"\n\u001b[0;32m--> 735\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exec_group\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_sync_params_from_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet/lib/python3.6/site-packages/mxnet/module/executor_group.py\u001b[0m in \u001b[0;36mupdate_metric\u001b[0;34m(self, eval_metric, labels)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0mlabels_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_slice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             \u001b[0meval_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_bind_ith_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet/lib/python3.6/site-packages/mxnet/metric.py\u001b[0m in \u001b[0;36mupdate_dict\u001b[0;34m(self, label, pred)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet/lib/python3.6/site-packages/mxnet/metric.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, labels, preds)\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpred_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0mpred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0mpred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1550\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   1553\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit(traindata_iterator,\n",
    "          eval_data=evaldata_iterator,\n",
    "          optimizer=mx.optimizer.Adam(rescale_grad=1/batch_size),\n",
    "          optimizer_params={'learning_rate': 0.1, 'momentum': 0.9},\n",
    "          eval_metric='acc',\n",
    "          batch_end_callback = mx.callback.Speedometer(batch_size, 10),\n",
    "          num_epoch=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mel_spectrogram:\n",
      "mel [(2, 100, 80)]\n",
      "postprocess:\n",
      "postprocess [(2, 100, 1025)]\n"
     ]
    }
   ],
   "source": [
    "mel_spectrogram = mx.sym.Variable(\"mel_spectrogram\")\n",
    "mel_spectrogram_shape = (2,100,80)\n",
    "print(\"mel_spectrogram:\")\n",
    "mel_arg_shapes, mel_out_shapes, mel_aux_shapes=mel_spectrogram.infer_shape(mel_spectrogram=mel_spectrogram_shape)\n",
    "#print(\"pre_arg_shapes\",pre_arg_shapes)\n",
    "print(\"mel\",mel_out_shapes)\n",
    "#print(\"pre_aux_shapes\",pre_aux_shapes)\n",
    "\n",
    "print(\"postprocess:\")\n",
    "pre_arg_shapes, pre_out_shapes, pre_aux_shapes=postprocess(mel_spectrogram).infer_shape(mel_spectrogram=mel_spectrogram_shape)\n",
    "#print(\"pre_arg_shapes\",pre_arg_shapes)\n",
    "print(\"postprocess\",pre_out_shapes)\n",
    "#print(\"pre_aux_shapes\",pre_aux_shapes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predicted_frames=mx.sym.zeros((1,80))\n",
    "full_frame=mx.sym.zeros((1,80))\n",
    "\n",
    "spectrograms_count=5 #dummy value\n",
    "for i in range(spectrograms_count):\n",
    "    predicted_frames,decoder_state = decoder(predicted_frames,decoder_state,reduction_factor)\n",
    "    full_frame=mx.sym.concat(full_frame,predicted_frames)\n",
    "\n",
    "spectral_magnitude=CBHG(full_frame, 8, emb_size, 80)\n",
    "\n",
    "graph=mx.viz.plot_network(\n",
    "    spectral_magnitude,\n",
    "    save_format='pdf',\n",
    "    title='decoder')\n",
    "graph.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
