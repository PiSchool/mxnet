{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TACOTRON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import nd, autograd\n",
    "from IPython.display import clear_output\n",
    "ctx= mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vocab():\n",
    "    vocab = \"EG abcdefghijklmnopqrstuvwxyz'\" # E: Empty. ignore G\n",
    "    char2idx = {char:idx for idx, char in enumerate(vocab)}\n",
    "    idx2char = {idx:char for idx, char in enumerate(vocab)}\n",
    "    return char2idx, idx2char  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FC-256-ReLU → Dropout(0.5) → FC-128-ReLU → Dropout(0.5)\n",
    "\"\"\"\n",
    "def prenet_pass(data):\n",
    "    fc1 = mx.symbol.FullyConnected(data=data, num_hidden=emb_size)\n",
    "    act1 = mx.symbol.Activation(data=fc1, act_type='relu')\n",
    "    drop1 = mx.symbol.Dropout(act1, p=0.5)\n",
    "    \n",
    "    fc2 = mx.symbol.FullyConnected(data=data, num_hidden=emb_size//2)\n",
    "    act2 = mx.symbol.Activation(data=fc2, act_type='relu')\n",
    "    prenet_output = mx.symbol.Dropout(act2, p=0.5)\n",
    "    \n",
    "    return prenet_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# banco di filtri convolutivi. Vengono creati K filtri con kernel 1D di dimensione:k \n",
    "def conv1dBank(conv_input, K):\n",
    "    conv=mx.sym.Convolution(data=conv_input, kernel=(1,1), num_filter=emb_size//2)\n",
    "    (conv, mean, var) = mx.sym.BatchNorm(data=conv, output_mean_var=True)\n",
    "    conv = mx.sym.Activation(data=conv, act_type='relu')\n",
    "    for k in range(2, K+1):\n",
    "        convi = mx.sym.Convolution(data=conv_input, kernel=(k,1), num_filter=emb_size//2)\n",
    "        (convi, mean, var) = mx.sym.BatchNorm(data=convi, output_mean_var=True)\n",
    "        convi = mx.sym.Activation(data=convi, act_type='relu')\n",
    "        conv = mx.symbol.concat(conv,convi)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# highway\n",
    "def highway_layer(data):\n",
    "    H= mx.symbol.Activation(data=mx.symbol.FullyConnected(data=data, num_hidden=emb_size//2), act_type=\"relu\")\n",
    "    T= mx.symbol.Activation(data=mx.symbol.FullyConnected(data=data, num_hidden=emb_size//2, bias=mx.sym.Variable('bias') ), act_type=\"sigmoid\")\n",
    "    return  H * T + data * (1.0 - T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GRULayer(data, bidirectional_mode=True, size=emb_size):\n",
    "    net = mx.sym.RNN(data=data,bidirectional=bidirectional_mode,mode='gru', num_layers=1, state_size=size)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CBHG\n",
    "def CBHG(data,K,proj1_size,proj2_size):\n",
    "    #se si usa infer_shape su convbank dando la dimensione dell'input, viene dedotta la shape appunto \n",
    "    bank = conv1dBank(data,K)\n",
    "    poold_bank = mx.sym.Pooling(data=bank, pool_type='max', kernel=(2, 1), stride=(1,1)) #TOSTUDY: stride?\n",
    "\n",
    "    proj1 = mx.sym.Convolution(data=poold_bank, kernel=(3,1), num_filter=proj1_size)\n",
    "    (proj1, proj1_mean, proj1_var) = mx.sym.BatchNorm(data=proj1, output_mean_var=True) #TOSTUDY: does the symbol encapsule the mean and var too?\n",
    "    proj1 = mx.sym.Activation(data=proj1, act_type='relu')#Is it ok to declare/use again the same variable?  \n",
    "\n",
    "    proj2 = mx.sym.Convolution(proj1, kernel=(3,1), num_filter=proj2_size)\n",
    "    (proj2, proj2_mean, proj2_var) = mx.sym.BatchNorm(data=proj2, output_mean_var=True)\n",
    "    \n",
    "    residual= proj2 + data\n",
    "\n",
    "    for i in range(4):\n",
    "        residual = highway_layer(residual)\n",
    "    highway_pass = residual\n",
    "   \n",
    "    encoded = GRULayer(highway_pass, size=emb_size//2)\n",
    "\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoder\n",
    "def encoder(data):\n",
    "    char2index, index2char = load_vocab()\n",
    "    \n",
    "    onehot = mx.sym.one_hot(data,len(char2index))\n",
    "    embed_vector = mx.sym.Embedding(data=onehot, input_dim=100, output_dim=emb_size)\n",
    "    prenet_output = prenet_pass(embed_vector)\n",
    "    return CBHG(prenet_output,16, emb_size//2, emb_size//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# decoder\n",
    "def decoder(data,encoder_context):\n",
    "    prenet_output = prenet_pass(data) # ???\n",
    "    \n",
    "    #attended = attention(prenet_output, encoder_context, num_inputs=emb_size) # pseudo shit\n",
    "    \n",
    "    gru1 = GRULayer(encoder_context,bidirectional_mode=False, size=emb_size)\n",
    "    gru2 = GRULayer(encoder_context,bidirectional_mode=False, size=emb_size)\n",
    "    \n",
    "    rnn_output = mx.symbol.Activation(\n",
    "        data=mx.symbol.FullyConnected(data=encoder_context + gru1 + gru2, num_hidden=emb_size),\n",
    "        act_type=\"relu\"\n",
    "    )\n",
    "    \n",
    "    return CBHG(rnn_output,8, emb_size, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_hidden = 256\n",
    "\n",
    "emb_size=256;\n",
    "\n",
    "\n",
    "text = mx.sym.Variable('text')\n",
    "spectrogram = mx.sym.Variable('spectrogram')\n",
    "\n",
    "encoded = encoder(data)\n",
    "spectrogram_output = decoder(spectrogram,encoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2> Attention model part </h2>\n",
    "\n",
    "Need to figure out what is going on here.\n",
    "\n",
    "\"memory\" part = data (embeddings, apples, whatever)-> prenet -> encoder-CBHG   \n",
    "\n",
    "keithito implementation: \n",
    "\n",
    "      # Attention\n",
    "      attention_cell = AttentionWrapper(\n",
    "        DecoderPrenetWrapper(GRUCell(256), is_training),\n",
    "        BahdanauAttention(256, encoder_outputs),\n",
    "        alignment_history=True,\n",
    "        output_attention=False)                                                  # [N, T_in, 256]\n",
    "first arg: a RNN cell \n",
    "second: attention mechanism\n",
    "\n",
    "both AttentionWrapper and BahdanauAttention comes from tf.contrib.seq2seq package\n",
    "    \n",
    "The DecoderPrenetWrapper:\n",
    "<div style=\"background-color:gray\">\n",
    "<code style=\"background-color:gray\">\n",
    "class DecoderPrenetWrapper(RNNCell):\n",
    "  '''Runs RNN inputs through a prenet before sending them to the cell.'''\n",
    "  \n",
    "  bla bla bla\n",
    "  \n",
    "  <b>def call(self, inputs, state):\n",
    "    prenet_out = prenet(inputs, self._is_training, scope='decoder_prenet')\n",
    "    return self._cell(prenet_out, state)\n",
    "  </b>  \n",
    "  bla bla bla\n",
    "</code></div>\n",
    "\n",
    "So it just send data to the prenet. Data comes from encoder_cbhg.\n",
    "    The AttentionWrapper, wraps a attention mechanism (Bahdanau) that look at a memory to query (the encoder output) and a net (GRU 256) that is the attention net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mx.viz.plot_network(encoder(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
