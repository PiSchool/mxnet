{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TACOTRON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De/Encoder pre net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How it's implemented on TF:\n",
    "\n",
    "    def prenet(inputs, is_training=True, scope=\"prenet\", reuse=None):\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            outputs = tf.layers.dense(inputs, units=hp.embed_size, activation=tf.nn.relu, name=\"dense1\")\n",
    "            outputs = tf.nn.dropout(outputs, keep_prob=.5 if is_training==True else 1., name=\"dropout1\")\n",
    "            outputs = tf.layers.dense(outputs, units=hp.embed_size//2, activation=tf.nn.relu, name=\"dense2\")\n",
    "            outputs = tf.nn.dropout(outputs, keep_prob=.5 if is_training==True else 1., name=\"dropout2\") \n",
    "        return outputs # (N, T, num_units/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import nd, autograd\n",
    "from mxnet import gluon\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx= mx.cpu()\n",
    "num_hidden = 256\n",
    "K=16\n",
    "emb_size=256;\n",
    "\n",
    "data = mx.sym.Variable('data')\n",
    "emb = mx.sym.Embedding(data=data, input_dim=100, output_dim=emb_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def prenet():\n",
    "    prenet = gluon.nn.Sequential() #consider to use HybridSequential\n",
    "    with prenet.name_scope():\n",
    "        prenet.add(gluon.nn.Dense(num_hidden,activation=\"relu\"))\n",
    "        prenet.add(gluon.nn.Dropout(0.5)) #how to use the condition \"if train\" same as TF?\n",
    "        prenet.add(gluon.nn.Dense(num_hidden//2,activation=\"relu\"))\n",
    "        prenet.add(gluon.nn.Dropout(0.5))\n",
    "    prenet.collect_params().initialize()\n",
    "    #prenet.initialize() seems the same\n",
    "    return prenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Dense(256, Activation(relu))\n",
      "  (1): Dropout(p = 0.5)\n",
      "  (2): Dense(128, Activation(relu))\n",
      "  (3): Dropout(p = 0.5)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "a = prenet()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#banco di filtri convolutivi. Vengono creati K filtri con kernel 1D di dimensione:k \n",
    "def conv1dBank(conv_input):\n",
    "    conv = mx.sym.Activation(data=mx.sym.Convolution(data=conv_input, kernel=(1,1), num_filter=emb_size//2), act_type='relu')\n",
    "    for k in range(2, K+1):\n",
    "        convi = mx.sym.Convolution(data=conv_input, kernel=(k,1), num_filter=emb_size//2)\n",
    "        (convi, mean, var) = mx.sym.BatchNorm(data=convi, output_mean_var=True) #TOSTUDY: batch norm?\n",
    "        convi = mx.sym.Activation(data=convi, act_type='relu')\n",
    "        conv = mx.symbol.concat(conv,convi)\n",
    "        #pooled_outputs.append(pooli)\n",
    "    return conv\n",
    "\n",
    "convbank=conv1dBank(emb)\n",
    "#mx.viz.plot_network(convbank)\n",
    "#se si usa infer_shape su convbank dando la dimensione dell'input, viene dedotta la shape appunto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    }
   ],
   "source": [
    "#highway #TOSTUDY\n",
    "def highway(data):\n",
    "    H= mx.symbol.Activation(data=mx.symbol.FullyConnected(data=data, num_hidden=emb_size//2), act_type=\"relu\")\n",
    "    T= mx.symbol.Activation(data=mx.symbol.FullyConnected(data=data, num_hidden=emb_size//2, bias=mx.sym.Variable('bias') ), act_type=\"sigmoid\")\n",
    "    return  H * T + data * (1.0 - T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded in comparison",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-692610b32ccd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mCBHG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-692610b32ccd>\u001b[0m in \u001b[0;36mCBHG\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mhw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhighway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCBHG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "... last 1 frames repeated, from the frame below ...\n",
      "\u001b[0;32m<ipython-input-34-692610b32ccd>\u001b[0m in \u001b[0;36mCBHG\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mhw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhighway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCBHG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded in comparison"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#CBHG\n",
    "def CBHG(data):\n",
    "    bank = conv1dBank(data)\n",
    "    poold_bank = mx.sym.Pooling(data=bank, pool_type='max', kernel=(2, 1), stride=(1,1)) #TOSTUDY: stride?\n",
    "\n",
    "    proj1 = mx.sym.Convolution(data=poold_bank, kernel=(3,1), num_filter=emb_size//2)\n",
    "    (proj1, proj1_mean, proj1_var) = mx.sym.BatchNorm(data=proj1, output_mean_var=True) #TOSTUDY: does the symbol encapsule the mean and var too?\n",
    "    proj1 = mx.sym.Activation(data=proj1, act_type='relu')#Is it ok to declare/use again the same variable?  \n",
    "\n",
    "    proj2 = mx.sym.Convolution(proj1, kernel=(3,1), num_filter=emb_size//2)\n",
    "    (proj2, proj2_mean, proj2_var) = mx.sym.BatchNorm(data=proj2, output_mean_var=True)\n",
    "    residual= proj2 + emb\n",
    "\n",
    "    hw = highway(residual)\n",
    "    mx.viz.plot_network(CBHG(emb) )\n",
    "    net = gluon.nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        for i in range(0,4):\n",
    "            #net.add(gluon.nn.Dense(100))\n",
    "            print(i)\n",
    "        \n",
    "    return net\n",
    "CBHG(emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
