{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TACOTRON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import nd, autograd\n",
    "from mxnet import gluon\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx= mx.cpu()\n",
    "num_hidden = 256\n",
    "\n",
    "emb_size=256;\n",
    "\n",
    "\n",
    "data = mx.sym.Variable('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab():\n",
    "    vocab = \"EG abcdefghijklmnopqrstuvwxyz'\" # E: Empty. ignore G\n",
    "    char2idx = {char:idx for idx, char in enumerate(vocab)}\n",
    "    idx2char = {idx:char for idx, char in enumerate(vocab)}\n",
    "    return char2idx, idx2char  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prenet_pass(data):\n",
    "    prenet = gluon.nn.Sequential() #consider to use HybridSequential\n",
    "    with prenet.name_scope():\n",
    "        prenet.add(gluon.nn.Dense(num_hidden,activation=\"relu\"))\n",
    "        prenet.add(gluon.nn.Dropout(0.5)) #how to use the condition \"if train\" same as TF?\n",
    "        prenet.add(gluon.nn.Dense(num_hidden//2,activation=\"relu\"))\n",
    "        prenet.add(gluon.nn.Dropout(0.5))\n",
    "    prenet.collect_params().initialize(mx.init.Normal(sigma=1.), ctx=ctx)\n",
    "    return prenet(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# banco di filtri convolutivi. Vengono creati K filtri con kernel 1D di dimensione:k \n",
    "def conv1dBank(conv_input, K):\n",
    "    conv=mx.sym.Convolution(data=conv_input, kernel=(1,1), num_filter=emb_size//2)\n",
    "    (conv, mean, var) = mx.sym.BatchNorm(data=conv, output_mean_var=True)\n",
    "    conv = mx.sym.Activation(data=conv, act_type='relu')\n",
    "    for k in range(2, K+1):\n",
    "        convi = mx.sym.Convolution(data=conv_input, kernel=(k,1), num_filter=emb_size//2)\n",
    "        (convi, mean, var) = mx.sym.BatchNorm(data=convi, output_mean_var=True)\n",
    "        convi = mx.sym.Activation(data=convi, act_type='relu')\n",
    "        conv = mx.symbol.concat(conv,convi)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# highway\n",
    "def highway_layer(data):\n",
    "    H= mx.symbol.Activation(data=mx.symbol.FullyConnected(data=data, num_hidden=emb_size//2), act_type=\"relu\")\n",
    "    T= mx.symbol.Activation(data=mx.symbol.FullyConnected(data=data, num_hidden=emb_size//2, bias=mx.sym.Variable('bias') ), act_type=\"sigmoid\")\n",
    "    return  H * T + data * (1.0 - T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BidirectionalGRULayer(data):\n",
    "    net = mx.sym.RNN(data=data,bidirectional=True,mode='gru', num_layers=1, state_size=emb_size//2)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CBHG\n",
    "def CBHG(data,emb):\n",
    "    #se si usa infer_shape su convbank dando la dimensione dell'input, viene dedotta la shape appunto \n",
    "    bank = conv1dBank(data,16)\n",
    "    poold_bank = mx.sym.Pooling(data=bank, pool_type='max', kernel=(2, 1), stride=(1,1)) #TOSTUDY: stride?\n",
    "\n",
    "    proj1 = mx.sym.Convolution(data=poold_bank, kernel=(3,1), num_filter=emb_size//2)\n",
    "    (proj1, proj1_mean, proj1_var) = mx.sym.BatchNorm(data=proj1, output_mean_var=True) #TOSTUDY: does the symbol encapsule the mean and var too?\n",
    "    proj1 = mx.sym.Activation(data=proj1, act_type='relu')#Is it ok to declare/use again the same variable?  \n",
    "\n",
    "    proj2 = mx.sym.Convolution(proj1, kernel=(3,1), num_filter=emb_size//2)\n",
    "    (proj2, proj2_mean, proj2_var) = mx.sym.BatchNorm(data=proj2, output_mean_var=True)\n",
    "    residual= proj2 + emb\n",
    "\n",
    "    for i in range(4):\n",
    "        residual = highway_layer(residual)\n",
    "    highway_pass = residual\n",
    "   \n",
    "    encoded = BidirectionalGRULayer(highway_pass)\n",
    "\n",
    "    return encoded\n",
    "\n",
    "#mx.viz.plot_network(CBHG(emb) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "def encoder(data):\n",
    "    char2index, index2char = load_vocab()\n",
    "    \n",
    "    onehot = mx.sym.one_hot(data,len(char2index))\n",
    "    emb = mx.sym.Embedding(data=onehot, input_dim=100, output_dim=emb_size)\n",
    "    prenetted_data = prenet_pass(emb)\n",
    "    return CBHG(prenetted_data,emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2> Attention model part </h2>\n",
    "\n",
    "Need to figure out what is going on here.\n",
    "\n",
    "\"memory\" part = data (embeddings, apples, whatever)-> prenet -> encoder-CBHG   \n",
    "\n",
    "keithito implementation: \n",
    "\n",
    "      # Attention\n",
    "      attention_cell = AttentionWrapper(\n",
    "        DecoderPrenetWrapper(GRUCell(256), is_training),\n",
    "        BahdanauAttention(256, encoder_outputs),\n",
    "        alignment_history=True,\n",
    "        output_attention=False)                                                  # [N, T_in, 256]\n",
    "first arg: a RNN cell \n",
    "second: attention mechanism\n",
    "\n",
    "both AttentionWrapper and BahdanauAttention comes from tf.contrib.seq2seq package\n",
    "    \n",
    "The DecoderPrenetWrapper:\n",
    "<div style=\"background-color:gray\">\n",
    "<code style=\"background-color:gray\">\n",
    "class DecoderPrenetWrapper(RNNCell):\n",
    "  '''Runs RNN inputs through a prenet before sending them to the cell.'''\n",
    "  \n",
    "  bla bla bla\n",
    "  \n",
    "  <b>def call(self, inputs, state):\n",
    "    prenet_out = prenet(inputs, self._is_training, scope='decoder_prenet')\n",
    "    return self._cell(prenet_out, state)\n",
    "  </b>  \n",
    "  bla bla bla\n",
    "</code></div>\n",
    "\n",
    "So it just send data to the prenet. Data comes from encoder_cbhg.\n",
    "    The AttentionWrapper, wraps a attention mechanism (Bahdanau) that look at a memory to query (the encoder output) and a net (GRU 256) that is the attention net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"). You may not\n",
    "# use this file except in compliance with the License. A copy of the License\n",
    "# is located at\n",
    "#\n",
    "#     http://aws.amazon.com/apache2.0/\n",
    "#\n",
    "# or in the \"license\" file accompanying this file. This file is distributed on\n",
    "# an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n",
    "# express or implied. See the License for the specific language governing\n",
    "# permissions and limitations under the License.\n",
    "\n",
    "\"\"\"\n",
    "Implementations of different attention mechanisms in sequence-to-sequence models.\n",
    "\"\"\"\n",
    "import logging\n",
    "from typing import Callable, NamedTuple, Optional, Tuple\n",
    "\n",
    "from sockeye import config\n",
    "from sockeye import constants as C\n",
    "from sockeye import coverage\n",
    "from sockeye import layers\n",
    "from sockeye import utils\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class AttentionConfig(config.Config):\n",
    "    \"\"\"\n",
    "    Attention configuration.\n",
    "\n",
    "    :param type: Attention name.\n",
    "    :param num_hidden: Number of hidden units for attention networks.\n",
    "    :param input_previous_word: Feeds the previous target embedding into the attention mechanism.\n",
    "    :param source_num_hidden: Number of hidden units of the source.\n",
    "    :param query_num_hidden: Number of hidden units of the query.\n",
    "    :param layer_normalization: Apply layer normalization to MLP attention.\n",
    "    :param config_coverage: Optional coverage configuration.\n",
    "    :param num_heads: Number of attention heads. Only used for Multi-head dot attention.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 type: str,\n",
    "                 num_hidden: int,\n",
    "                 input_previous_wordn: bool,\n",
    "              atta   source_num_hidden: int,\n",
    "                 query_num_hidden: int,\n",
    "                 layer_normalization: bool,\n",
    "                 config_coverage: Optional[coverage.CoverageConfig] = None,\n",
    "                 num_heads: Optional[int] = None) -> None:\n",
    "        super().__init__()\n",
    "        self.type = type\n",
    "        self.num_hidden = num_hidden\n",
    "        self.input_previous_word = input_previous_word\n",
    "        self.source_num_hidden = source_num_hidden\n",
    "        self.query_num_hidden = query_num_hidden\n",
    "        self.layer_normalization = layer_normalization\n",
    "        self.config_coverage = config_coverage\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "\n",
    "def get_attention(config: AttentionConfig, max_seq_len: int) -> 'Attention':\n",
    "    \"\"\"\n",
    "    Returns an Attention instance based on attention_type.\n",
    "\n",
    "    :param config: Attention configuration.\n",
    "    :param max_seq_len: Maximum length of source sequences.\n",
    "    :return: Instance of Attention.\n",
    "    \"\"\"\n",
    "    if config.type == C.ATT_BILINEAR:\n",
    "        if config.input_previous_word:\n",
    "            logger.warning(\"bilinear attention does not support input_previous_word\")\n",
    "        return BilinearAttention(config.query_num_hidden)\n",
    "    elif config.type == C.ATT_DOT:\n",
    "        return DotAttention(config.input_previous_word, config.source_num_hidden, config.query_num_hidden,\n",
    "                            config.num_hidden)\n",
    "    elif config.type == C.ATT_MH_DOT:\n",
    "        utils.check_condition(config.num_heads is not None, \"%s requires setting num-heads.\" % C.ATT_MH_DOT)\n",
    "        return MultiHeadDotAttention(config.input_previous_word,\n",
    "                                     num_hidden=config.num_hidden,\n",
    "                                     heads=config.num_heads)\n",
    "    elif config.type == C.ATT_DOT_SCALED:\n",
    "        return DotAttention(config.input_previous_word, config.source_num_hidden, config.query_num_hidden,\n",
    "                            config.num_hidden, scale=config.num_hidden ** -0.5)\n",
    "    elif config.type == C.ATT_FIXED:\n",
    "        return EncoderLastStateAttention(config.input_previous_word)\n",
    "    elif config.type == C.ATT_LOC:\n",
    "        return LocationAttention(config.input_previous_word, max_seq_len)\n",
    "    elif config.type == C.ATT_MLP:\n",
    "        return MlpAttention(input_previous_word=config.input_previous_word,\n",
    "                            attention_num_hidden=config.num_hidden,\n",
    "                            layer_normalization=config.layer_normalization)\n",
    "    elif config.type == C.ATT_COV:\n",
    "        return MlpAttention(input_previous_word=config.input_previous_word,\n",
    "                            attention_num_hidden=config.num_hidden,\n",
    "                            layer_normalization=config.layer_normalization,\n",
    "                            config_coverage=config.config_coverage)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown attention type %s\" % config.type)\n",
    "\n",
    "\n",
    "AttentionInput = NamedTuple('AttentionInput', [('seq_idx', int), ('query', mx.sym.Symbol)])\n",
    "\"\"\"\n",
    "Input to attention callables.\n",
    "\n",
    ":param seq_idx: Decoder time step / sequence index.\n",
    ":param query: Query input to attention mechanism, e.g. decoder hidden state (plus previous word).\n",
    "\"\"\"\n",
    "\n",
    "AttentionState = NamedTuple('AttentionState', [\n",
    "    ('context', mx.sym.Symbol),\n",
    "    ('probs', mx.sym.Symbol),\n",
    "    ('dynamic_source', mx.sym.Symbol),\n",
    "])\n",
    "\"\"\"\n",
    "Results returned from attention callables.\n",
    "\n",
    ":param context: Context vector (Bahdanau et al, 15). Shape: (batch_size, encoder_num_hidden)\n",
    ":param probs: Attention distribution over source encoder states. Shape: (batch_size, source_seq_len).\n",
    ":param dynamic_source: Dynamically updated source encoding.\n",
    "       Shape: (batch_size, source_seq_len, dynamic_source_num_hidden)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Attention(object):\n",
    "    \"\"\"\n",
    "    Generic attention interface that returns a callable for attending to source states.\n",
    "\n",
    "    :param input_previous_word: Feed the previous target embedding into the attention mechanism.\n",
    "    :param dynamic_source_num_hidden: Number of hidden units of dynamic source encoding update mechanism.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_previous_word: bool,\n",
    "                 dynamic_source_num_hidden: int = 1,\n",
    "                 prefix: str = C.ATTENTION_PREFIX) -> None:\n",
    "        self.dynamic_source_num_hidden = dynamic_source_num_hidden\n",
    "        self._input_previous_word = input_previous_word\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def on(self, source: mx.sym.Symbol, source_length: mx.sym.Symbol, source_seq_len: int) -> Callable:\n",
    "        \"\"\"\n",
    "        Returns callable to be used for recurrent attention in a sequence decoder.\n",
    "        The callable is a recurrent function of the form:\n",
    "        AttentionState = attend(AttentionInput, AttentionState).\n",
    "\n",
    "        :param source: Shape: (batch_size, seq_len, encoder_num_hidden).\n",
    "        :param source_length: Shape: (batch_size,).\n",
    "        :param source_seq_len: Maximum length of source sequences.\n",
    "        :return: Attention callable.\n",
    "        \"\"\"\n",
    "\n",
    "        def attend(att_input: AttentionInput, att_state: AttentionState) -> AttentionState:\n",
    "            \"\"\"\n",
    "            Returns updated attention state given attention input and current attention state.\n",
    "\n",
    "            :param att_input: Attention input as returned by make_input().\n",
    "            :param att_state: Current attention state\n",
    "            :return: Updated attention state.\n",
    "            \"\"\"\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        return attend\n",
    "\n",
    "    def get_initial_state(self, source_length: mx.sym.Symbol, source_seq_len: int) -> AttentionState:\n",
    "        \"\"\"\n",
    "        Returns initial attention state. Dynamic source encoding is initialized with zeros.\n",
    "\n",
    "        :param source_length: Source length. Shape: (batch_size,).\n",
    "        :param source_seq_len: Maximum length of source sequences.\n",
    "        \"\"\"\n",
    "        dynamic_source = mx.sym.expand_dims(mx.sym.expand_dims(mx.sym.zeros_like(source_length), axis=1), axis=2)\n",
    "        # dynamic_source: (batch_size, source_seq_len, num_hidden_dynamic_source)\n",
    "        dynamic_source = mx.sym.broadcast_to(dynamic_source, shape=(0, source_seq_len, self.dynamic_source_num_hidden))\n",
    "        return AttentionState(context=None, probs=None, dynamic_source=dynamic_source)\n",
    "\n",
    "    def make_input(self,\n",
    "                   seq_idx: int,\n",
    "                   word_vec_prev: mx.sym.Symbol,\n",
    "                   decoder_state: mx.sym.Symbol) -> AttentionInput:\n",
    "        \"\"\"\n",
    "        Returns AttentionInput to be fed into the attend callable returned by the on() method.\n",
    "\n",
    "        :param seq_idx: Decoder time step.\n",
    "        :param word_vec_prev: Embedding of previously predicted ord\n",
    "        :param decoder_state: Current decoder state\n",
    "        :return: Attention input.\n",
    "        \"\"\"\n",
    "        query = decoder_state\n",
    "        if self._input_previous_word:\n",
    "            # (batch_size, num_target_embed + rnn_num_hidden)\n",
    "            query = mx.sym.concat(word_vec_prev, decoder_state, dim=1,\n",
    "                                  name='%sconcat_prev_word_%d' % (self.prefix, seq_idx))\n",
    "        return AttentionInput(seq_idx=seq_idx, query=query)\n",
    "\n",
    "\n",
    "class BilinearAttention(Attention):\n",
    "    \"\"\"\n",
    "    Bilinear attention based on Luong et al. 2015.\n",
    "\n",
    "    :math:`score(h_t, h_s) = h_t^T \\\\mathbf{W} h_s`\n",
    "\n",
    "    For implementation reasons we modify to:\n",
    "\n",
    "    :math:`score(h_t, h_s) = h_s^T \\\\mathbf{W} h_t`\n",
    "\n",
    "    :param num_hidden: Number of hidden units the source will be projected to.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_hidden: int) -> None:\n",
    "        super().__init__(False)\n",
    "        self.num_hidden = num_hidden\n",
    "        self.s2t_weight = mx.sym.Variable(\"%ss2t_weight\" % self.prefix)\n",
    "\n",
    "    def on(self, source: mx.sym.Symbol, source_length: mx.sym.Symbol, source_seq_len: int) -> Callable:\n",
    "        \"\"\"\n",
    "        Returns callable to be used for recurrent attention in a sequence decoder.\n",
    "        The callable is a recurrent function of the form:\n",
    "        AttentionState = attend(AttentionInput, AttentionState).\n",
    "\n",
    "        :param source: Shape: (batch_size, seq_len, encoder_num_hidden).\n",
    "        :param source_length: Shape: (batch_size,).\n",
    "        :param source_seq_len: Maximum length of source sequences.\n",
    "        :return: Attention callable.\n",
    "        \"\"\"\n",
    "\n",
    "        # (batch_size, seq_len, self.num_hidden)\n",
    "        source_hidden = mx.sym.FullyConnected(data=source,\n",
    "                                              weight=self.s2t_weight,\n",
    "                                              num_hidden=self.num_hidden,\n",
    "                                              no_bias=True,\n",
    "                                              flatten=False,\n",
    "                                              name=\"%ssource_hidden_fc\" % self.prefix)\n",
    "\n",
    "        def attend(att_input: AttentionInput, att_state: AttentionState) -> AttentionState:\n",
    "            \"\"\"\n",
    "            Returns updated attention state given attention input and current attention state.\n",
    "\n",
    "            :param att_input: Attention input as returned by make_input().\n",
    "            :param att_state: Current attention state\n",
    "            :return: Updated attention state.\n",
    "            \"\"\"\n",
    "            # (batch_size, decoder_num_hidden, 1)\n",
    "            query = mx.sym.expand_dims(att_input.query, axis=2)\n",
    "\n",
    "            # in:  (batch_size, source_seq_len, self.num_hidden) X (batch_size, self.num_hidden, 1)\n",
    "            # out: (batch_size, source_seq_len, 1).\n",
    "            attention_scores = mx.sym.batch_dot(lhs=source_hidden, rhs=query, name=\"%sbatch_dot\" % self.prefix)\n",
    "\n",
    "            context, attention_probs = get_context_and_attention_probs(source, source_length, attention_scores)\n",
    "\n",
    "            return AttentionState(context=context,\n",
    "                                  probs=attention_probs,\n",
    "                                  dynamic_source=att_state.dynamic_source)\n",
    "\n",
    "        return attend\n",
    "\n",
    "\n",
    "class DotAttention(Attention):\n",
    "    \"\"\"\n",
    "    Attention mechanism with dot product between encoder and decoder hidden states [Luong et al. 2015].\n",
    "\n",
    "    :math:`score(h_t, h_s) =  \\\\langle h_t, h_s \\\\rangle`\n",
    "\n",
    "    :math:`a = softmax(score(*, h_s))`\n",
    "\n",
    "    If rnn_num_hidden != num_hidden, states are projected with additional parameters to num_hidden.\n",
    "\n",
    "    :math:`score(h_t, h_s) = \\\\langle \\\\mathbf{W}_t h_t, \\\\mathbf{W}_s h_s \\\\rangle`\n",
    "\n",
    "    :param input_previous_word: Feed the previous target embedding into the attention mechanism.\n",
    "    :param rnn_num_hidden: Number of hidden units in encoder/decoder RNNs.\n",
    "    :param num_hidden: Number of hidden units.\n",
    "    :param scale: Optionally scale query before dot product [Vaswani et al, 2017].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_previous_word: bool,\n",
    "                 source_num_hidden: int,\n",
    "                 query_num_hidden: int,\n",
    "                 num_hidden: int,\n",
    "                 scale: Optional[float] = None) -> None:\n",
    "        super().__init__(input_previous_word)\n",
    "        self.project_source = source_num_hidden != num_hidden\n",
    "        self.project_query = query_num_hidden != num_hidden\n",
    "        self.num_hidden = num_hidden\n",
    "        self.scale = scale\n",
    "\n",
    "        self.s2h_weight = mx.sym.Variable(\"%ss2h_weight\" % self.prefix) if self.project_source else None\n",
    "        self.t2h_weight = mx.sym.Variable(\"%st2h_weight\" % self.prefix) if self.project_query else None\n",
    "\n",
    "    def on(self, source: mx.sym.Symbol, source_length: mx.sym.Symbol, source_seq_len: int) -> Callable:\n",
    "        \"\"\"\n",
    "        Returns callable to be used for recurrent attention in a sequence decoder.\n",
    "        The callable is a recurrent function of the form:\n",
    "        AttentionState = attend(AttentionInput, AttentionState).\n",
    "\n",
    "        :param source: Shape: (batch_size, seq_len, encoder_num_hidden).\n",
    "        :param source_length: Shape: (batch_size,).\n",
    "        :param source_seq_len: Maximum length of source sequences.\n",
    "        :return: Attention callable.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.project_source:\n",
    "            # (batch_size, seq_len, self.num_hidden)\n",
    "            source_hidden = mx.sym.FullyConnected(data=source,\n",
    "                                                  weight=self.s2h_weight,\n",
    "                                                  num_hidden=self.num_hidden,\n",
    "                                                  no_bias=True,\n",
    "                                                  flatten=False,\n",
    "                                                  name=\"%ssource_hidden_fc\" % self.prefix)\n",
    "        else:\n",
    "            source_hidden = source\n",
    "\n",
    "        def attend(att_input: AttentionInput, att_state: AttentionState) -> AttentionState:\n",
    "            \"\"\"\n",
    "            Returns updated attention state given attention input and current attention state.\n",
    "\n",
    "            :param att_input: Attention input as returned by make_input().\n",
    "            :param att_state: Current attention state\n",
    "            :return: Updated attention state.\n",
    "            \"\"\"\n",
    "            query = att_input.query\n",
    "            if self.project_query:\n",
    "                # query: (batch_size, self.num_hidden)\n",
    "                query = mx.sym.FullyConnected(data=query,\n",
    "                                              weight=self.t2h_weight,\n",
    "                                              num_hidden=self.num_hidden,\n",
    "                                              no_bias=True, name=\"%squery_hidden_fc\" % self.prefix)\n",
    "\n",
    "            # scale down dot product by sqrt(num_hidden) [Vaswani et al, 17]\n",
    "            if self.scale is not None:\n",
    "                query = query * self.scale\n",
    "\n",
    "            # (batch_size, decoder_num_hidden, 1)\n",
    "            expanded_decoder_state = mx.sym.expand_dims(query, axis=2)\n",
    "\n",
    "            # batch_dot: (batch, M, K) X (batch, K, N) –> (batch, M, N).\n",
    "            # (batch_size, seq_len, 1)\n",
    "            attention_scores = mx.sym.batch_dot(lhs=source_hidden, rhs=expanded_decoder_state,\n",
    "                                                name=\"%sbatch_dot\" % self.prefix)\n",
    "\n",
    "            context, attention_probs = get_context_and_attention_probs(source, source_length, attention_scores)\n",
    "            return AttentionState(context=context,\n",
    "                                  probs=attention_probs,\n",
    "                                  dynamic_source=att_state.dynamic_source)\n",
    "\n",
    "        return attend\n",
    "\n",
    "\n",
    "class MultiHeadDotAttention(Attention):\n",
    "    \"\"\"\n",
    "    Dot product attention with multiple heads as proposed in Vaswani et al, Attention is all you need.\n",
    "    Can be used with a RecurrentDecoder.\n",
    "\n",
    "    :param input_previous_word: Feed the previous target embedding into the attention mechanism.\n",
    "    :param num_hidden: Number of hidden units.\n",
    "    :param heads: Number of attention heads / independently computed attention scores.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_previous_word: bool,\n",
    "                 num_hidden: int,\n",
    "                 heads: int) -> None:\n",
    "        super().__init__(input_previous_word)\n",
    "        utils.check_condition(num_hidden % heads == 0,\n",
    "                              \"Number of heads (%d) must divide attention depth (%d)\" % (heads, num_hidden))\n",
    "        self.num_hidden = num_hidden\n",
    "        self.heads = heads\n",
    "        self.num_hidden_per_head = self.num_hidden // self.heads\n",
    "        self.s2h_weight = mx.sym.Variable(\"%ss2h_weight\" % self.prefix)\n",
    "        self.s2h_bias = mx.sym.Variable(\"%ss2h_bias\" % self.prefix)\n",
    "        self.t2h_weight = mx.sym.Variable(\"%st2h_weight\" % self.prefix)\n",
    "        self.t2h_bias = mx.sym.Variable(\"%st2h_bias\" % self.prefix)\n",
    "        self.h2o_weight = mx.sym.Variable(\"%sh2o_weight\" % self.prefix)\n",
    "        self.h2o_bias = mx.sym.Variable(\"%sh2o_bias\" % self.prefix)\n",
    "\n",
    "    def on(self, source: mx.sym.Symbol, source_length: mx.sym.Symbol, source_seq_len: int) -> Callable:\n",
    "        \"\"\"\n",
    "        Returns callable to be used for recurrent attention in a sequence decoder.\n",
    "        The callable is a recurrent function of the form:\n",
    "        AttentionState = attend(AttentionInput, AttentionState).\n",
    "\n",
    "        :param source: Shape: (batch_size, seq_len, encoder_num_hidden).\n",
    "        :param source_length: Shape: (batch_size,).\n",
    "        :param source_seq_len: Maximum length of source sequences.\n",
    "        :return: Attention callable.\n",
    "        \"\"\"\n",
    "        # (batch, length, num_hidden * 2)\n",
    "        source_hidden = mx.sym.FullyConnected(data=source,\n",
    "                                              weight=self.s2h_weight,\n",
    "                                              bias=self.s2h_bias,\n",
    "                                              num_hidden=self.num_hidden * 2,\n",
    "                                              flatten=False,\n",
    "                                              name=\"%ssource_hidden_fc\" % self.prefix)\n",
    "        # split keys and values\n",
    "        # (batch, length, num_hidden)\n",
    "        # pylint: disable=unbalanced-tuple-unpacking\n",
    "        keys, values = mx.sym.split(data=source_hidden, num_outputs=2, axis=2)\n",
    "\n",
    "        # (batch*heads, length, num_hidden/head)\n",
    "        keys = layers.split_heads(keys, source_seq_len, self.heads)\n",
    "        values = layers.split_heads(values, source_seq_len, self.heads)\n",
    "\n",
    "        def attend(att_input: AttentionInput, att_state: AttentionState) -> AttentionState:\n",
    "            \"\"\"\n",
    "            Returns updated attention state given attention input and current attention state.\n",
    "\n",
    "            :param att_input: Attention input as returned by make_input().\n",
    "            :param att_state: Current attention state\n",
    "            :return: Updated attention state.\n",
    "            \"\"\"\n",
    "            # (batch, num_hidden)\n",
    "            query = mx.sym.FullyConnected(data=att_input.query,\n",
    "                                          weight=self.t2h_weight, bias=self.t2h_bias,\n",
    "                                          num_hidden=self.num_hidden, name=\"%squery_hidden_fc\" % self.prefix)\n",
    "            # (batch, length, heads, num_hidden/head)\n",
    "            query = mx.sym.reshape(query, shape=(0, 1, self.heads, self.num_hidden_per_head))\n",
    "            # (batch, heads, num_hidden/head, length)\n",
    "            query = mx.sym.transpose(query, axes=(0, 2, 3, 1))\n",
    "            # (batch * heads, num_hidden/head, 1)\n",
    "            query = mx.sym.reshape(query, shape=(-3, self.num_hidden_per_head, 1))\n",
    "\n",
    "            # scale dot product\n",
    "            query = query * (self.num_hidden_per_head ** -0.5)\n",
    "\n",
    "            # (batch*heads, length, num_hidden/head) X (batch*heads, num_hidden/head, 1)\n",
    "            #   -> (batch*heads, length, 1)\n",
    "            attention_scores = mx.sym.batch_dot(lhs=keys, rhs=query, name=\"%sdot\" % self.prefix)\n",
    "\n",
    "            # (batch*heads, 1)\n",
    "            lengths = layers.broadcast_to_heads(source_length, self.heads)\n",
    "\n",
    "            # context: (batch*heads, num_hidden/head)\n",
    "            # attention_probs: (batch*heads, length)\n",
    "            context, attention_probs = get_context_and_attention_probs(values, lengths, attention_scores)\n",
    "\n",
    "            # combine heads\n",
    "            # (batch*heads, 1, num_hidden/head)\n",
    "            context = mx.sym.expand_dims(context, axis=1)\n",
    "            # (batch, 1, num_hidden)\n",
    "            context = layers.combine_heads(context, length=1, heads=self.heads)\n",
    "            # (batch, num_hidden)\n",
    "            context = mx.sym.reshape(context, shape=(-3, -1))\n",
    "\n",
    "            # (batch, heads, length)\n",
    "            attention_probs = mx.sym.reshape(data=attention_probs, shape=(-4, -1, self.heads, source_seq_len))\n",
    "            # just average over distributions\n",
    "            attention_probs = mx.sym.mean(attention_probs, axis=1, keepdims=False)\n",
    "\n",
    "            return AttentionState(context=context,\n",
    "                                  probs=attention_probs,\n",
    "                                  dynamic_source=att_state.dynamic_source)\n",
    "\n",
    "        return attend\n",
    "\n",
    "\n",
    "class EncoderLastStateAttention(Attention):\n",
    "    \"\"\"\n",
    "    Always returns the last encoder state independent of the query vector.\n",
    "    Equivalent to no attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def on(self, source: mx.sym.Symbol, source_length: mx.sym.Symbol, source_seq_len: int) -> Callable:\n",
    "        \"\"\"\n",
    "        Returns callable to be used for recurrent attention in a sequence decoder.\n",
    "        The callable is a recurrent function of the form:\n",
    "        AttentionState = attend(AttentionInput, AttentionState).\n",
    "\n",
    "        :param source: Shape: (batch_size, seq_len, encoder_num_hidden).\n",
    "        :param source_length: Shape: (batch_size,).\n",
    "        :param source_seq_len: Maximum length of source sequences.\n",
    "        :return: Attention callable.\n",
    "        \"\"\"\n",
    "        source = mx.sym.swapaxes(source, dim1=0, dim2=1)\n",
    "        encoder_last_state = mx.sym.SequenceLast(data=source, sequence_length=source_length,\n",
    "                                                 use_sequence_length=True)\n",
    "        fixed_probs = mx.sym.one_hot(source_length - 1, depth=source_seq_len)\n",
    "\n",
    "        def attend(att_input: AttentionInput, att_state: AttentionState) -> AttentionState:\n",
    "            return AttentionState(context=encoder_last_state,\n",
    "                                  probs=fixed_probs,\n",
    "                                  dynamic_source=att_state.dynamic_source)\n",
    "\n",
    "        return attend\n",
    "\n",
    "\n",
    "class LocationAttention(Attention):\n",
    "    \"\"\"\n",
    "    Attends to locations in the source [Luong et al, 2015]\n",
    "\n",
    "    :math:`a_t = softmax(\\\\mathbf{W}_a h_t)` for decoder hidden state at time t.\n",
    "\n",
    "    :note: :math:`\\\\mathbf{W}_a` is of shape (max_source_seq_len, decoder_num_hidden).\n",
    "\n",
    "    :param input_previous_word: Feed the previous target embedding into the attention mechanism.\n",
    "    :param max_source_seq_len: Maximum length of source sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_previous_word: bool,\n",
    "                 max_source_seq_len: int) -> None:\n",
    "        super().__init__(input_previous_word)\n",
    "        self.max_source_seq_len = max_source_seq_len\n",
    "        self.location_weight = mx.sym.Variable(\"%sloc_weight\" % self.prefix)\n",
    "        self.location_bias = mx.sym.Variable(\"%sloc_bias\" % self.prefix)\n",
    "\n",
    "    def on(self, source: mx.sym.Symbol, source_length: mx.sym.Symbol, source_seq_len: int) -> Callable:\n",
    "        \"\"\"\n",
    "        Returns callable to be used for recurrent attention in a sequence decoder.\n",
    "        The callable is a recurrent function of the form:\n",
    "        AttentionState = attend(AttentionInput, AttentionState).\n",
    "\n",
    "        :param source: Shape: (batch_size, seq_len, encoder_num_hidden).\n",
    "        :param source_length: Shape: (batch_size,).\n",
    "        :param source_seq_len: Maximum length of source sequences.\n",
    "        :return: Attention callable.\n",
    "        \"\"\"\n",
    "\n",
    "        def attend(att_input: AttentionInput, att_state: AttentionState) -> AttentionState:\n",
    "            \"\"\"\n",
    "            Returns updated attention state given attention input and current attention state.\n",
    "\n",
    "            :param att_input: Attention input as returned by make_input().\n",
    "            :param att_state: Current attention state\n",
    "            :return: Updated attention state.\n",
    "            \"\"\"\n",
    "            # attention_scores: (batch_size, seq_len)\n",
    "            attention_scores = mx.sym.FullyConnected(data=att_input.query,\n",
    "                                                     num_hidden=self.max_source_seq_len,\n",
    "                                                     weight=self.location_weight,\n",
    "                                                     bias=self.location_bias)\n",
    "\n",
    "            # attention_scores: (batch_size, seq_len)\n",
    "            attention_scores = mx.sym.slice_axis(data=attention_scores,\n",
    "                                                 axis=1,\n",
    "                                                 begin=0,\n",
    "                                                 end=source_seq_len)\n",
    "\n",
    "            # attention_scores: (batch_size, seq_len, 1)\n",
    "            attention_scores = mx.sym.expand_dims(data=attention_scores, axis=2)\n",
    "\n",
    "            context, attention_probs = get_context_and_attention_probs(source, source_length, attention_scores)\n",
    "            return AttentionState(context=context,\n",
    "                                  probs=attention_probs,\n",
    "                                  dynamic_source=att_state.dynamic_source)\n",
    "\n",
    "        return attend\n",
    "\n",
    "\n",
    "class MlpAttention(Attention):\n",
    "    \"\"\"\n",
    "    Attention computed through a one-layer MLP with num_hidden units [Luong et al, 2015].\n",
    "\n",
    "    :math:`score(h_t, h_s) = \\\\mathbf{W}_a tanh(\\\\mathbf{W}_c [h_t, h_s] + b)`\n",
    "\n",
    "    :math:`a = softmax(score(*, h_s))`\n",
    "\n",
    "    Optionally, if attention_coverage_type is not None, attention uses dynamic source encoding ('coverage' mechanism)\n",
    "    as in Tu et al. (2016): Modeling Coverage for Neural Machine Translation.\n",
    "\n",
    "    :math:`score(h_t, h_s) = \\\\mathbf{W}_a tanh(\\\\mathbf{W}_c [h_t, h_s, c_s] + b)`\n",
    "\n",
    "    :math:`c_s` is the decoder time-step dependent source encoding which is updated using the current\n",
    "    decoder state.\n",
    "\n",
    "    :param input_previous_word: Feed the previous target embedding into the attention mechanism.\n",
    "    :param attention_num_hidden: Number of hidden units.\n",
    "    :param attention_coverage_type: The type of update for the dynamic source encoding.\n",
    "           If None, no dynamic source encoding is done.\n",
    "    :param attention_coverage_num_hidden: Number of hidden units for coverage attention.\n",
    "    :param prefix: Layer name prefix.\n",
    "    :param layer_normalization: If true, normalizes hidden layer outputs before tanh activation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_previous_word: bool,\n",
    "                 attention_num_hidden: int,\n",
    "                 layer_normalization: bool = False,\n",
    "                 config_coverage: Optional[coverage.CoverageConfig] = None) -> None:\n",
    "        dynamic_source_num_hidden = 1 if config_coverage is None else config_coverage.num_hidden\n",
    "        super().__init__(input_previous_word=input_previous_word,\n",
    "                         dynamic_source_num_hidden=dynamic_source_num_hidden)\n",
    "        self.attention_num_hidden = attention_num_hidden\n",
    "        # input (encoder) to hidden\n",
    "        self.att_e2h_weight = mx.sym.Variable(\"%se2h_weight\" % self.prefix)\n",
    "        # input (query) to hidden\n",
    "        self.att_q2h_weight = mx.sym.Variable(\"%sq2h_weight\" % self.prefix)\n",
    "        # hidden to score\n",
    "        self.att_h2s_weight = mx.sym.Variable(\"%sh2s_weight\" % self.prefix)\n",
    "        # coverage\n",
    "        self.coverage = coverage.get_coverage(config_coverage) if config_coverage is not None else None\n",
    "        # dynamic source (coverage) weights and settings\n",
    "        # input (coverage) to hidden\n",
    "        self.att_c2h_weight = mx.sym.Variable(\"%sc2h_weight\" % self.prefix) if config_coverage is not None else None\n",
    "        # layer normalization\n",
    "        self._ln = layers.LayerNormalization(num_hidden=attention_num_hidden,\n",
    "                                             prefix=\"%snorm\" % self.prefix) if layer_normalization else None\n",
    "\n",
    "    def on(self, source: mx.sym.Symbol, source_length: mx.sym.Symbol, source_seq_len: int) -> Callable:\n",
    "        \"\"\"\n",
    "        Returns callable to be used for recurrent attention in a sequence decoder.\n",
    "        The callable is a recurrent function of the form:\n",
    "        AttentionState = attend(AttentionInput, AttentionState).\n",
    "\n",
    "        :param source: Shape: (batch_size, seq_len, encoder_num_hidden).\n",
    "        :param source_length: Shape: (batch_size,).\n",
    "        :param source_seq_len: Maximum length of source sequences.\n",
    "        :return: Attention callable.\n",
    "        \"\"\"\n",
    "\n",
    "        coverage_func = self.coverage.on(source, source_length, source_seq_len) if self.coverage else None\n",
    "\n",
    "        # (batch_size, seq_len, attention_num_hidden)\n",
    "        source_hidden = mx.sym.FullyConnected(data=source,\n",
    "                                              weight=self.att_e2h_weight,\n",
    "                                              num_hidden=self.attention_num_hidden,\n",
    "                                              no_bias=True,\n",
    "                                              flatten=False,\n",
    "                                              name=\"%ssource_hidden_fc\" % self.prefix)\n",
    "\n",
    "        def attend(att_input: AttentionInput, att_state: AttentionState) -> AttentionState:\n",
    "            \"\"\"\n",
    "            Returns updated attention state given attention input and current attention state.\n",
    "\n",
    "            :param att_input: Attention input as returned by make_input().\n",
    "            :param att_state: Current attention state\n",
    "            :return: Updated attention state.\n",
    "            \"\"\"\n",
    "\n",
    "            # (batch_size, attention_num_hidden)\n",
    "            query_hidden = mx.sym.FullyConnected(data=att_input.query,\n",
    "                                                 weight=self.att_q2h_weight,\n",
    "                                                 num_hidden=self.attention_num_hidden,\n",
    "                                                 no_bias=True,\n",
    "                                                 name=\"%squery_hidden\" % self.prefix)\n",
    "\n",
    "            # (batch_size, 1, attention_num_hidden)\n",
    "            query_hidden = mx.sym.expand_dims(data=query_hidden,\n",
    "                                              axis=1,\n",
    "                                              name=\"%squery_hidden_expanded\" % self.prefix)\n",
    "\n",
    "            attention_hidden_lhs = source_hidden\n",
    "            if self.coverage:\n",
    "                # (batch_size, seq_len, attention_num_hidden)\n",
    "                dynamic_hidden = mx.sym.FullyConnected(data=att_state.dynamic_source,\n",
    "                                                       weight=self.att_c2h_weight,\n",
    "                                                       num_hidden=self.attention_num_hidden,\n",
    "                                                       no_bias=True,\n",
    "                                                       flatten=False,\n",
    "                                                       name=\"%sdynamic_source_hidden_fc\" % self.prefix)\n",
    "\n",
    "                # (batch_size, seq_len, attention_num_hidden\n",
    "                attention_hidden_lhs = dynamic_hidden + source_hidden\n",
    "\n",
    "            # (batch_size, seq_len, attention_num_hidden)\n",
    "            attention_hidden = mx.sym.broadcast_add(lhs=attention_hidden_lhs, rhs=query_hidden,\n",
    "                                                    name=\"%squery_plus_input\" % self.prefix)\n",
    "\n",
    "            # (batch_size * seq_len, attention_num_hidden)\n",
    "            attention_hidden = mx.sym.reshape(data=attention_hidden,\n",
    "                                              shape=(-3, -1),\n",
    "                                              name=\"%squery_plus_input_before_fc\" % self.prefix)\n",
    "\n",
    "            if self._ln is not None:\n",
    "                attention_hidden = self._ln.normalize(attention_hidden)\n",
    "\n",
    "            # (batch_size * seq_len, attention_num_hidden)\n",
    "            attention_hidden = mx.sym.Activation(attention_hidden, act_type=\"tanh\",\n",
    "                                                 name=\"%shidden\" % self.prefix)\n",
    "\n",
    "            # (batch_size * seq_len, 1)\n",
    "            attention_scores = mx.sym.FullyConnected(data=attention_hidden,\n",
    "                                                     weight=self.att_h2s_weight,\n",
    "                                                     num_hidden=1,\n",
    "                                                     no_bias=True,\n",
    "                                                     name=\"%sraw_att_score_fc\" % self.prefix)\n",
    "\n",
    "            # (batch_size, seq_len, 1)\n",
    "            attention_scores = mx.sym.reshape(attention_scores,\n",
    "                                              shape=(-1, source_seq_len, 1),\n",
    "                                              name=\"%sraw_att_score_fc\" % self.prefix)\n",
    "\n",
    "            context, attention_probs = get_context_and_attention_probs(source, source_length, attention_scores)\n",
    "\n",
    "            dynamic_source = att_state.dynamic_source\n",
    "            if self.coverage:\n",
    "                # update dynamic source encoding\n",
    "                # Note: this is a slight change to the Tu et al, 2016 paper: input to the coverage update\n",
    "                # is the attention input query, not the previous decoder state.\n",
    "                dynamic_source = coverage_func(prev_hidden=att_input.query,\n",
    "                                               attention_prob_scores=attention_probs,\n",
    "                                               prev_coverage=att_state.dynamic_source)\n",
    "\n",
    "            return AttentionState(context=context,\n",
    "                                  probs=attention_probs,\n",
    "                                  dynamic_source=dynamic_source)\n",
    "\n",
    "        return attend\n",
    "\n",
    "\n",
    "def mask_attention_scores(logits: mx.sym.Symbol,\n",
    "                          length: mx.sym.Symbol) -> mx.sym.Symbol:\n",
    "    \"\"\"\n",
    "    Masks attention scores according to sequence length.\n",
    "\n",
    "    :param logits: Shape: (batch_size, seq_len, 1).\n",
    "    :param length: Shape: (batch_size,).\n",
    "    :return: Masked logits: (batch_size, seq_len, 1).\n",
    "    \"\"\"\n",
    "    # TODO: Masking with 0-1 mask, to avoid the multiplication\n",
    "    logits = mx.sym.swapaxes(data=logits, dim1=0, dim2=1)\n",
    "    logits = mx.sym.SequenceMask(data=logits,\n",
    "                                 use_sequence_length=True,\n",
    "                                 sequence_length=length,\n",
    "                                 value=-99999999.)\n",
    "    # (batch_size, seq_len, 1)\n",
    "    return mx.sym.swapaxes(data=logits, dim1=0, dim2=1)\n",
    "\n",
    "\n",
    "def get_context_and_attention_probs(values: mx.sym.Symbol,\n",
    "                                    length: mx.sym.Symbol,\n",
    "                                    logits: mx.sym.Symbol) -> Tuple[mx.sym.Symbol, mx.sym.Symbol]:\n",
    "    \"\"\"\n",
    "    Returns context vector and attention probabilities\n",
    "    via a weighted sum over values.\n",
    "\n",
    "    :param values: Shape: (batch_size, seq_len, encoder_num_hidden).\n",
    "    :param length: Shape: (batch_size,).\n",
    "    :param logits: Shape: (batch_size, seq_len, 1).\n",
    "    :return: context: (batch_size, encoder_num_hidden), attention_probs: (batch_size, seq_len).\n",
    "    \"\"\"\n",
    "    # (batch_size, seq_len, 1)\n",
    "    logits = mask_attention_scores(logits, length)\n",
    "\n",
    "    # (batch_size, seq_len, 1)\n",
    "    probs = mx.sym.softmax(logits, axis=1, name='attention_softmax')\n",
    "\n",
    "    # batch_dot: (batch, M, K) X (batch, K, N) –> (batch, M, N).\n",
    "    # (batch_size, seq_len, num_hidden) X (batch_size, seq_len, 1) -> (batch_size, num_hidden, 1)\n",
    "    context = mx.sym.batch_dot(lhs=values, rhs=probs, transpose_a=True)\n",
    "    # (batch_size, encoder_num_hidden, 1)-> (batch_size, encoder_num_hidden)\n",
    "    context = mx.sym.reshape(data=context, shape=(0, 0))\n",
    "    probs = mx.sym.reshape(data=probs, shape=(0, 0))\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class AttentionConfig(config.Config):\n",
    "    \"\"\"\n",
    "    Attention configuration.\n",
    "\n",
    "    :param type: Attention name.\n",
    "    :param num_hidden: Number of hidden units for attention networks.\n",
    "    :param input_previous_word: Feeds the previous target embedding into the attention mechanism.\n",
    "    :param source_num_hidden: Number of hidden units of the source.\n",
    "    :param query_num_hidden: Number of hidden units of the query.\n",
    "    :param layer_normalization: Apply layer normalization to MLP attention.\n",
    "    :param config_coverage: Optional coverage configuration.\n",
    "    :param num_heads: Number of attention heads. Only used for Multi-head dot attention.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 type: str,\n",
    "                 num_hidden: int,\n",
    "                 input_previous_wordn: bool,\n",
    "                 source_num_hidden: int,\n",
    "                 query_num_hidden: int,\n",
    "                 layer_normalization: bool,\n",
    "                 config_coverage: Optional[coverage.CoverageConfig] = None,\n",
    "                 num_heads: Optional[int] = None) -> None:\n",
    "        super().__init__()\n",
    "        self.type = type\n",
    "        self.num_hidden = num_hidden\n",
    "        self.input_previous_word = input_previous_word\n",
    "        self.source_num_hidden = source_num_hidden\n",
    "        self.query_num_hidden = query_num_hidden\n",
    "        self.layer_normalization = layer_normalization\n",
    "        self.config_coverage = config_coverage\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "\n",
    "def get_attention(config: AttentionConfig, max_seq_len: int) -> 'Attention':\n",
    "    \"\"\"\n",
    "    Returns an Attention instance based on attention_type.\n",
    "\n",
    "    :param config: Attention configuration.\n",
    "    :param max_seq_len: Maximum length of source sequences.\n",
    "    :return: Instance of Attention.\n",
    "    \"\"\"\n",
    "    if config.type == C.ATT_BILINEAR:\n",
    "        if config.input_previous_word:\n",
    "            logger.warning(\"bilinear attention does not support input_previous_word\")\n",
    "        return BilinearAttention(config.query_num_hidden)\n",
    "    elif config.type == C.ATT_DOT:\n",
    "        return DotAttention(config.input_previous_word, config.source_num_hidden, config.query_num_hidden,\n",
    "                            config.num_hidden)\n",
    "    elif config.type == C.ATT_MH_DOT:\n",
    "        utils.check_condition(config.num_heads is not None, \"%s requires setting num-heads.\" % C.ATT_MH_DOT)\n",
    "        return MultiHeadDotAttention(config.input_previous_word,\n",
    "                                     num_hidden=config.num_hidden,\n",
    "                                     heads=config.num_heads)\n",
    "    elif config.type == C.ATT_DOT_SCALED:\n",
    "        return DotAttention(config.input_previous_word, config.source_num_hidden, config.query_num_hidden,\n",
    "                            config.num_hidden, scale=config.num_hidden ** -0.5)\n",
    "    elif config.type == C.ATT_FIXED:\n",
    "        return EncoderLastStateAttention(config.input_previous_word)\n",
    "    elif config.type == C.ATT_LOC:\n",
    "        return LocationAttention(config.input_previous_word, max_seq_len)\n",
    "    elif config.type == C.ATT_MLP:\n",
    "        return MlpAttention(input_previous_word=config.input_previous_word,\n",
    "                            attention_num_hidden=config.num_hidden,\n",
    "                            layer_normalization=config.layer_normalization)\n",
    "    elif config.type == C.ATT_COV:\n",
    "        return MlpAttention(input_previous_word=config.input_previous_word,\n",
    "                            attention_num_hidden=config.num_hidden,\n",
    "                            layer_normalization=config.layer_normalization,\n",
    "                            config_coverage=config.config_coverage)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown attention type %s\" % config.type)\n",
    "\n",
    "\n",
    "AttentionInput = NamedTuple('AttentionInput', [('seq_idx', int), ('query', mx.sym.Symbol)])\n",
    "\"\"\"\n",
    "Input to attention callables.\n",
    "\n",
    ":param seq_idx: Decoder time step / sequence index.\n",
    ":param query: Query input to attention mechanism, e.g. decoder hidden state (plus previous word).\n",
    "\"\"\"\n",
    "\n",
    "AttentionState = NamedTuple('AttentionState', [\n",
    "    ('context', mx.sym.Symbol),\n",
    "    ('probs', mx.sym.Symbol),\n",
    "    ('dynamic_source', mx.sym.Symbol),\n",
    "])\n",
    "\"\"\"\n",
    "Results returned from attention callables.\n",
    "\n",
    ":param context: Context vector (Bahdanau et al, 15). Shape: (batch_size, encoder_num_hidden)\n",
    ":param probs: Attention distribution over source encoder states. Shape: (batch_size, source_seq_len).\n",
    ":param dynamic_source: Dynamically updated source encoding.\n",
    "       Shape: (batch_size, source_seq_len, dynamic_source_num_hidden)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Attention(object):\n",
    "    \"\"\"\n",
    "    Generic attention interface that returns a callable for attending to source states.\n",
    "\n",
    "    :param input_previous_word: Feed the previous target embedding into the attention mechanism.\n",
    "    :param dynamic_source_num_hidden: Number of hidden units of dynamic source encoding update mechanism.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_previous_word: bool,\n",
    "                 dynamic_source_num_hidden: int = 1,\n",
    "                 prefix: str = C.ATTENTION_PREFIX) -> None:\n",
    "        self.dynamic_source_num_hidden = dynamic_source_num_hidden\n",
    "        self._input_previous_word = input_previous_word\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def on(self, source: mx.sym.Symbol, source_length: mx.sym.Symbol, source_seq_len: int) -> Callable:\n",
    "        \"\"\"\n",
    "        Returns callable to be used for recurrent attention in a sequence decoder.\n",
    "        The callable is a recurrent function of the form:\n",
    "        AttentionState = attend(AttentionInput, AttentionState).\n",
    "\n",
    "        :param source: Shape: (batch_size, seq_len, encoder_num_hidden).\n",
    "        :param source_length: Shape: (batch_size,).\n",
    "        :param source_seq_len: Maximum length of source sequences.\n",
    "        :return: Attention callable.\n",
    "        \"\"\"\n",
    "\n",
    "        def attend(att_input: AttentionInput, att_state: AttentionState) -> AttentionState:\n",
    "            \"\"\"\n",
    "            Returns updated attention state given attention input and current attention state.\n",
    "\n",
    "            :param att_input: Attention input as returned by make_input().\n",
    "            :param att_state: Current attention state\n",
    "            :return: Updated attention state.\n",
    "            \"\"\"\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        return attend\n",
    "\n",
    "    def get_initial_state(self, source_length: mx.sym.Symbol, source_seq_len: int) -> AttentionState:\n",
    "        \"\"\"\n",
    "        Returns initial attention state. Dynamic source encoding is initialized with zeros.\n",
    "\n",
    "        :param source_length: Source length. Shape: (batch_size,).\n",
    "        :param source_seq_len: Maximum length of source sequences.\n",
    "        \"\"\"\n",
    "        dynamic_source = mx.sym.expand_dims(mx.sym.expand_dims(mx.sym.zeros_like(source_length), axis=1), axis=2)\n",
    "        # dynamic_source: (batch_size, source_seq_len, num_hidden_dynamic_source)\n",
    "        dynamic_source = mx.sym.broadcast_to(dynamic_source, shape=(0, source_seq_len, self.dynamic_source_num_hidden))\n",
    "        return AttentionState(context=None, probs=None, dynamic_source=dynamic_source)\n",
    "\n",
    "    def make_input(self,\n",
    "                   seq_idx: int,\n",
    "                   word_vec_prev: mx.sym.Symbol,\n",
    "                   decoder_state: mx.sym.Symbol) -> AttentionInput:\n",
    "        \"\"\"\n",
    "        Returns AttentionInput to be fed into the attend callable returned by the on() method.\n",
    "\n",
    "        :param seq_idx: Decoder time step.\n",
    "        :param word_vec_prev: Embedding of previously predicted ord\n",
    "        :param decoder_state: Current decoder state\n",
    "        :return: Attention input.\n",
    "        \"\"\"\n",
    "        query = decoder_state\n",
    "        if self._input_previous_word:\n",
    "            # (batch_size, num_target_embed + rnn_num_hidden)\n",
    "            query = mx.sym.concat(word_vec_prev, decoder_state, dim=1,\n",
    "                                  name='%sconcat_prev_word_%d' % (self.prefix, seq_idx))\n",
    "        return AttentionInput(seq_idx=seq_idx, query=query)\n",
    "\n",
    "\n",
    "class BilinearAttention(Attention):\n",
    "    \"\"\"\n",
    "    Bilinear attention based on Luong et al. 2015.\n",
    "\n",
    "    :math:`score(h_t, h_s) = h_t^T \\\\mathbf{W} h_s`\n",
    "\n",
    "    For implementation reasons we modify to:\n",
    "\n",
    "    :math:`score(h_t, h_s) = h_s^T \\\\mathbf{W} h_t`\n",
    "\n",
    "    :param num_hidden: Number of hidden units the source will be projected to.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_hidden: int) -> None:\n",
    "        super().__init__(False)\n",
    "        self.num_hidden = num_hidden\n",
    "        self.s2t_weight = mx.sym.Variable(\"%ss2t_weight\" % self.prefix)\n",
    "\n",
    "    def on(self, source: mx.sym.Symbol, source_length: mx.sym.Symbol, source_seq_len: int) -> Callable:\n",
    "        \"\"\"\n",
    "        Returns callable to be used for recurrent attention in a sequence decoder.\n",
    "        The callable is a recurrent function of the form:\n",
    "        AttentionState = attend(AttentionInput, AttentionState).\n",
    "\n",
    "        :param source: Shape: (batch_size, seq_len, encoder_num_hidden).\n",
    "        :param source_length: Shape: (batch_size,).\n",
    "        :param source_seq_len: Maximum length of source sequences.\n",
    "        :return: Attention callable.\n",
    "        \"\"\"\n",
    "\n",
    "        # (batch_size, seq_len, self.num_hidden)\n",
    "        source_hidden = mx.sym.FullyConnected(data=source,\n",
    "                                              weight=self.s2t_weight,\n",
    "                                              num_hidden=self.num_hidden,\n",
    "                                              no_bias=True,\n",
    "                                              flatten=False,\n",
    "                                              name=\"%ssource_hidden_fc\" % self.prefix)\n",
    "\n",
    "        def attend(att_input: AttentionInput, att_state: AttentionState) -> AttentionState:\n",
    "            \"\"\"\n",
    "            Returns updated attention state given attention input and current attention state.\n",
    "\n",
    "            :param att_input: Attention input as returned by make_input().\n",
    "            :param att_state: Current attention state\n",
    "            :return: Updated attention state.\n",
    "            \"\"\"\n",
    "            # (batch_size, decoder_num_hidden, 1)\n",
    "            query = mx.sym.expand_dims(att_input.query, axis=2)\n",
    "\n",
    "            # in:  (batch_size, source_seq_len, self.num_hidden) X (batch_size, self.num_hidden, 1)\n",
    "            # out: (batch_size, source_seq_len, 1).\n",
    "            attention_scores = mx.sym.batch_dot(lhs=source_hidden, rhs=query, name=\"%sbatch_dot\" % self.prefix)\n",
    "\n",
    "            context, attention_probs = get_context_and_attention_probs(source, source_length, attention_scores)\n",
    "\n",
    "            return AttentionState(context=context,\n",
    "                                  probs=attention_probs,\n",
    "                                  dynamic_source=att_state.dynamic_source)\n",
    "\n",
    "        return attend\n",
    "\n",
    "\n",
    "class DotAttention(Attention):\n",
    "    \"\"\"\n",
    "    Attention mechanism with dot product between encoder and decoder hidden states [Luong et al. 2015].\n",
    "\n",
    "    :math:`score(h_t, h_s) =  \\\\langle h_t, h_s \\\\rangle`\n",
    "\n",
    "    :math:`a = softmax(score(*, h_s))`\n",
    "\n",
    "    If rnn_num_hidden != num_hidden, states are projected with additional parameters to num_hidden.\n",
    "\n",
    "    :math:`score(h_t, h_s) = \\\\langle \\\\mathbf{W}_t h_t, \\\\mathbf{W}_s h_s \\\\rangle`\n",
    "\n",
    "    :param input_previous_word: Feed the previous target embedding into the attention mechanism.\n",
    "    :param rnn_num_hidden: Number of hidden units in encoder/decoder RNNs.\n",
    "    :param num_hidden: Number of hidden units.\n",
    "    :param scale: Optionally scale query before dot product [Vaswani et al, 2017].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_previous_word: bool,\n",
    "                 source_num_hidden: int,\n",
    "                 query_num_hidden: int,\n",
    "                 num_hidden: int,\n",
    "                 scale: Optional[float] = None) -> None:\n",
    "        super().__init__(input_previous_word)\n",
    "        self.project_source = source_num_hidden != num_hidden\n",
    "        self.project_query = query_num_hidden != num_hidden\n",
    "        self.num_hidden = num_hidden\n",
    "        self.scale = scale\n",
    "\n",
    "        self.s2h_weight = mx.sym.Variable(\"%ss2h_weight\" % self.prefix) if self.project_source else None\n",
    "        self.t2h_weight = mx.sym.Variable(\"%st2h_weight\" % self.prefix) if self.project_query else None\n",
    "\n",
    "    def on(self, source: mx.sym.Symbol, source_length: mx.sym.Symbol, source_seq_len: int) -> Callable:\n",
    "        \"\"\"\n",
    "        Returns callable to be used for recurrent attention in a sequence decoder.\n",
    "        The callable is a recurrent function of the form:\n",
    "        AttentionState = attend(AttentionInput, AttentionState).\n",
    "\n",
    "        :param source: Shape: (batch_size, seq_len, encoder_num_hidden).\n",
    "        :param source_length: Shape: (batch_size,).\n",
    "        :param source_seq_len: Maximum length of source sequences.\n",
    "        :return: Attention callable.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.project_source:\n",
    "            # (batch_size, seq_len, self.num_hidden)\n",
    "            source_hidden = mx.sym.FullyConnected(data=source,\n",
    "                                                  weight=self.s2h_weight,\n",
    "                                                  num_hidden=self.num_hidden,\n",
    "                                                  no_bias=True,\n",
    "                                                  flatten=False,\n",
    "                                                  name=\"%ssource_hidden_fc\" % self.prefix)\n",
    "        else:\n",
    "            source_hidden = source\n",
    "\n",
    "        def attend(att_input: AttentionInput, att_state: AttentionState) -> AttentionState:\n",
    "            \"\"\"\n",
    "            Returns updated attention state given attention input and current attention state.\n",
    "\n",
    "            :param att_input: Attention input as returned by make_input().\n",
    "            :param att_state: Current attention state\n",
    "            :return: Updated attention state.\n",
    "            \"\"\"\n",
    "            query = att_input.query\n",
    "            if self.project_query:\n",
    "                # query: (batch_size, self.num_hidden)\n",
    "                query = mx.sym.FullyConnected(data=query,\n",
    "                                              weight=self.t2h_weight,\n",
    "                                              num_hidden=self.num_hidden,\n",
    "                                              no_bias=True, name=\"%squery_hidden_fc\" % self.prefix)\n",
    "\n",
    "            # scale down dot product by sqrt(num_hidden) [Vaswani et al, 17]\n",
    "            if self.scale is not None:\n",
    "                query = query * self.scale\n",
    "\n",
    "            # (batch_size, decoder_num_hidden, 1)\n",
    "            expanded_decoder_state = mx.sym.expand_dims(query, axis=2)\n",
    "\n",
    "            # batch_dot: (batch, M, K) X (batch, K, N) –> (batch, M, N).\n",
    "            # (batch_size, seq_len, 1)\n",
    "            attention_scores = mx.sym.batch_dot(lhs=source_hidden, rhs=expanded_decoder_state,\n",
    "                                                name=\"%sbatch_dot\" % self.prefix)\n",
    "\n",
    "            context, attention_probs = get_context_and_attention_probs(source, source_length, attention_scores)\n",
    "            return AttentionState(context=context,\n",
    "                                  probs=attention_probs,\n",
    "                                  dynamic_source=att_state.dynamic_source)\n",
    "\n",
    "        return attend\n",
    "\n",
    "\n",
    "class MultiHeadDotAttention(Attention):\n",
    "    \"\"\"\n",
    "    Dot product attention with multiple heads as proposed in Vaswani et al, Attention is all you need.\n",
    "    Can be used with a RecurrentDecoder.\n",
    "\n",
    "    :param input_previous_word: Feed the previous target embedding into the attention mechanism.\n",
    "    :param num_hidden: Number of hidden units.\n",
    "    :param heads: Number of attention heads / independently computed attention scores.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_previous_word: bool,\n",
    "                 num_hidden: int,\n",
    "                 heads: int) -> None:\n",
    "        super().__init__(input_previous_word)\n",
    "        utils.check_condition(num_hidden % heads == 0,\n",
    "                              \"Number of heads (%d) must divide attention depth (%d)\" % (heads, num_hidden))\n",
    "        self.num_hidden = num_hidden\n",
    "        self.heads = heads\n",
    "        self.num_hidden_per_head = self.num_hidden // self.heads\n",
    "        self.s2h_weight = mx.sym.Variable(\"%ss2h_weight\" % self.prefix)\n",
    "        self.s2h_bias = mx.sym.Variable(\"%ss2h_bias\" % self.prefix)\n",
    "        self.t2h_weight = mx.sym.Variable(\"%st2h_weight\" % self.prefix)\n",
    "        self.t2h_bias = mx.sym.Variable(\"%st2h_bias\" % self.prefix)\n",
    "        self.h2o_weight = mx.sym.Variable(\"%sh2o_weight\" % self.prefix)\n",
    "        self.h2o_bias = mx.sym.Variable(\"%sh2o_bias\" % self.prefix)\n",
    "\n",
    "    def on(self, source: mx.sym.Symbol, source_length: mx.sym.Symbol, source_seq_len: int) -> Callable:\n",
    "        \"\"\"\n",
    "        Returns callable to be used for recurrent attention in a sequence decoder.\n",
    "        The callable is a recurrent function of the form:\n",
    "        AttentionState = attend(AttentionInput, AttentionState).\n",
    "\n",
    "        :param source: Shape: (batch_size, seq_len, encoder_num_hidden).\n",
    "        :param source_length: Shape: (batch_size,).\n",
    "        :param source_seq_len: Maximum length of source sequences.\n",
    "        :return: Attention callable.\n",
    "        \"\"\"\n",
    "        # (batch, length, num_hidden * 2)\n",
    "        source_hidden = mx.sym.FullyConnected(data=source,\n",
    "                                              weight=self.s2h_weight,\n",
    "                                              bias=self.s2h_bias,\n",
    "                                              num_hidden=self.num_hidden * 2,\n",
    "                                              flatten=False,\n",
    "                                              name=\"%ssource_hidden_fc\" % self.prefix)\n",
    "        # split keys and values\n",
    "        # (batch, length, num_hidden)\n",
    "        # pylint: disable=unbalanced-tuple-unpacking\n",
    "        keys, values = mx.sym.split(data=source_hidden, num_outputs=2, axis=2)\n",
    "\n",
    "        # (batch*heads, length, num_hidden/head)\n",
    "        keys = layers.split_heads(keys, source_seq_len, self.heads)\n",
    "        values = layers.split_heads(values, source_seq_len, self.heads)\n",
    "\n",
    "        def attend(att_input: AttentionInput, att_state: AttentionState) -> AttentionState:\n",
    "            \"\"\"\n",
    "            Returns updated attention state given attention input and current attention state.\n",
    "\n",
    "            :param att_input: Attention input as returned by make_input().\n",
    "            :param att_state: Current attention state\n",
    "            :return: Updated attention state.\n",
    "            \"\"\"\n",
    "            # (batch, num_hidden)\n",
    "            query = mx.sym.FullyConnected(data=att_input.query,\n",
    "                                          weight=self.t2h_weight, bias=self.t2h_bias,\n",
    "                                          num_hidden=self.num_hidden, name=\"%squery_hidden_fc\" % self.prefix)\n",
    "            # (batch, length, heads, num_hidden/head)\n",
    "            query = mx.sym.reshape(query, shape=(0, 1, self.heads, self.num_hidden_per_head))\n",
    "            # (batch, heads, num_hidden/head, length)\n",
    "            query = mx.sym.transpose(query, axes=(0, 2, 3, 1))\n",
    "            # (batch * heads, num_hidden/head, 1)\n",
    "            query = mx.sym.reshape(query, shape=(-3, self.num_hidden_per_head, 1))\n",
    "\n",
    "            # scale dot product\n",
    "            query = query * (self.num_hidden_per_head ** -0.5)\n",
    "\n",
    "            # (batch*heads, length, num_hidden/head) X (batch*heads, num_hidden/head, 1)\n",
    "            #   -> (batch*heads, length, 1)\n",
    "            attention_scores = mx.sym.batch_dot(lhs=keys, rhs=query, name=\"%sdot\" % self.prefix)\n",
    "\n",
    "            # (batch*heads, 1)\n",
    "            lengths = layers.broadcast_to_heads(source_length, self.heads)\n",
    "\n",
    "            # context: (batch*heads, num_hidden/head)\n",
    "            # attention_probs: (batch*heads, length)\n",
    "            context, attention_probs = get_context_and_attention_probs(values, lengths, attention_scores)\n",
    "\n",
    "            # combine heads\n",
    "            # (batch*heads, 1, num_hidden/head)\n",
    "            context = mx.sym.expand_dims(context, axis=1)\n",
    "            # (batch, 1, num_hidden)\n",
    "            context = layers.combine_heads(context, length=1, heads=self.heads)\n",
    "            # (batch, num_hidden)\n",
    "            context = mx.sym.reshape(context, shape=(-3, -1))\n",
    "\n",
    "            # (batch, heads, length)\n",
    "            attention_probs = mx.sym.reshape(data=attention_probs, shape=(-4, -1, self.heads, source_seq_len))\n",
    "            # just average over distributions\n",
    "            attention_probs = mx.sym.mean(attention_probs, axis=1, keepdims=False)\n",
    "\n",
    "            return AttentionState(context=context,\n",
    "                                  probs=attention_probs,\n",
    "                                  dynamic_source=att_state.dynamic_source)\n",
    "\n",
    "        return attend\n",
    "\n",
    "\n",
    "class EncoderLastStateAttention(Attention):\n",
    "    \"\"\"\n",
    "    Always returns the last encoder state independent of the query vector.\n",
    "    Equivalent to no attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def on(self, source: mx.sym.Symbol, source_length: mx.sym.Symbol, source_seq_len: int) -> Callable:\n",
    "        \"\"\"\n",
    "        Returns callable to be used for recurrent attention in a sequence decoder.\n",
    "        The callable is a recurrent function of the form:\n",
    "        AttentionState = attend(AttentionInput, AttentionState).\n",
    "\n",
    "        :param source: Shape: (batch_size, seq_len, encoder_num_hidden).\n",
    "        :param source_length: Shape: (batch_size,).\n",
    "        :param source_seq_len: Maximum length of source sequences.\n",
    "        :return: Attention callable.\n",
    "        \"\"\"\n",
    "        source = mx.sym.swapaxes(source, dim1=0, dim2=1)\n",
    "        encoder_last_state = mx.sym.SequenceLast(data=source, sequence_length=source_length,\n",
    "                                                 use_sequence_length=True)\n",
    "        fixed_probs = mx.sym.one_hot(source_length - 1, depth=source_seq_len)\n",
    "\n",
    "        def attend(att_input: AttentionInput, att_state: AttentionState) -> AttentionState:\n",
    "            return AttentionState(context=encoder_last_state,\n",
    "                                  probs=fixed_probs,\n",
    "                                  dynamic_source=att_state.dynamic_source)\n",
    "\n",
    "        return attend\n",
    "\n",
    "\n",
    "class LocationAttention(Attention):\n",
    "    \"\"\"\n",
    "    Attends to locations in the source [Luong et al, 2015]\n",
    "\n",
    "    :math:`a_t = softmax(\\\\mathbf{W}_a h_t)` for decoder hidden state at time t.\n",
    "\n",
    "    :note: :math:`\\\\mathbf{W}_a` is of shape (max_source_seq_len, decoder_num_hidden).\n",
    "\n",
    "    :param input_previous_word: Feed the previous target embedding into the attention mechanism.\n",
    "    :param max_source_seq_len: Maximum length of source sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_previous_word: bool,\n",
    "                 max_source_seq_len: int) -> None:\n",
    "        super().__init__(input_previous_word)\n",
    "        self.max_source_seq_len = max_source_seq_len\n",
    "        self.location_weight = mx.sym.Variable(\"%sloc_weight\" % self.prefix)\n",
    "        self.location_bias = mx.sym.Variable(\"%sloc_bias\" % self.prefix)\n",
    "\n",
    "    def on(self, source: mx.sym.Symbol, source_length: mx.sym.Symbol, source_seq_len: int) -> Callable:\n",
    "        \"\"\"\n",
    "        Returns callable to be used for recurrent attention in a sequence decoder.\n",
    "        The callable is a recurrent function of the form:\n",
    "        AttentionState = attend(AttentionInput, AttentionState).\n",
    "\n",
    "        :param source: Shape: (batch_size, seq_len, encoder_num_hidden).\n",
    "        :param source_length: Shape: (batch_size,).\n",
    "        :param source_seq_len: Maximum length of source sequences.\n",
    "        :return: Attention callable.\n",
    "        \"\"\"\n",
    "\n",
    "        def attend(att_input: AttentionInput, att_state: AttentionState) -> AttentionState:\n",
    "            \"\"\"\n",
    "            Returns updated attention state given attention input and current attention state.\n",
    "\n",
    "            :param att_input: Attention input as returned by make_input().\n",
    "            :param att_state: Current attention state\n",
    "            :return: Updated attention state.\n",
    "            \"\"\"\n",
    "            # attention_scores: (batch_size, seq_len)\n",
    "            attention_scores = mx.sym.FullyConnected(data=att_input.query,\n",
    "                                                     num_hidden=self.max_source_seq_len,\n",
    "                                                     weight=self.location_weight,\n",
    "                                                     bias=self.location_bias)\n",
    "\n",
    "            # attention_scores: (batch_size, seq_len)\n",
    "            attention_scores = mx.sym.slice_axis(data=attention_scores,\n",
    "                                                 axis=1,\n",
    "                                                 begin=0,\n",
    "                                                 end=source_seq_len)\n",
    "\n",
    "            # attention_scores: (batch_size, seq_len, 1)\n",
    "            attention_scores = mx.sym.expand_dims(data=attention_scores, axis=2)\n",
    "\n",
    "            context, attention_probs = get_context_and_attention_probs(source, source_length, attention_scores)\n",
    "            return AttentionState(context=context,\n",
    "                                  probs=attention_probs,\n",
    "                                  dynamic_source=att_state.dynamic_source)\n",
    "\n",
    "        return attend\n",
    "\n",
    "\n",
    "class MlpAttention(Attention):\n",
    "    \"\"\"\n",
    "    Attention computed through a one-layer MLP with num_hidden units [Luong et al, 2015].\n",
    "\n",
    "    :math:`score(h_t, h_s) = \\\\mathbf{W}_a tanh(\\\\mathbf{W}_c [h_t, h_s] + b)`\n",
    "\n",
    "    :math:`a = softmax(score(*, h_s))`\n",
    "\n",
    "    Optionally, if attention_coverage_type is not None, attention uses dynamic source encoding ('coverage' mechanism)\n",
    "    as in Tu et al. (2016): Modeling Coverage for Neural Machine Translation.\n",
    "\n",
    "    :math:`score(h_t, h_s) = \\\\mathbf{W}_a tanh(\\\\mathbf{W}_c [h_t, h_s, c_s] + b)`\n",
    "\n",
    "    :math:`c_s` is the decoder time-step dependent source encoding which is updated using the current\n",
    "    decoder state.\n",
    "\n",
    "    :param input_previous_word: Feed the previous target embedding into the attention mechanism.\n",
    "    :param attention_num_hidden: Number of hidden units.\n",
    "    :param attention_coverage_type: The type of update for the dynamic source encoding.\n",
    "           If None, no dynamic source encoding is done.\n",
    "    :param attention_coverage_num_hidden: Number of hidden units for coverage attention.\n",
    "    :param prefix: Layer name prefix.\n",
    "    :param layer_normalization: If true, normalizes hidden layer outputs before tanh activation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_previous_word: bool,\n",
    "                 attention_num_hidden: int,\n",
    "                 layer_normalization: bool = False,\n",
    "                 config_coverage: Optional[coverage.CoverageConfig] = None) -> None:\n",
    "        dynamic_source_num_hidden = 1 if config_coverage is None else config_coverage.num_hidden\n",
    "        super().__init__(input_previous_word=input_previous_word,\n",
    "                         dynamic_source_num_hidden=dynamic_source_num_hidden)\n",
    "        self.attention_num_hidden = attention_num_hidden\n",
    "        # input (encoder) to hidden\n",
    "        self.att_e2h_weight = mx.sym.Variable(\"%se2h_weight\" % self.prefix)\n",
    "        # input (query) to hidden\n",
    "        self.att_q2h_weight = mx.sym.Variable(\"%sq2h_weight\" % self.prefix)\n",
    "        # hidden to score\n",
    "        self.att_h2s_weight = mx.sym.Variable(\"%sh2s_weight\" % self.prefix)\n",
    "        # coverage\n",
    "        self.coverage = coverage.get_coverage(config_coverage) if config_coverage is not None else None\n",
    "        # dynamic source (coverage) weights and settings\n",
    "        # input (coverage) to hidden\n",
    "        self.att_c2h_weight = mx.sym.Variable(\"%sc2h_weight\" % self.prefix) if config_coverage is not None else None\n",
    "        # layer normalization\n",
    "        self._ln = layers.LayerNormalization(num_hidden=attention_num_hidden,\n",
    "                                             prefix=\"%snorm\" % self.prefix) if layer_normalization else None\n",
    "\n",
    "    def on(self, source: mx.sym.Symbol, source_length: mx.sym.Symbol, source_seq_len: int) -> Callable:\n",
    "        \"\"\"\n",
    "        Returns callable to be used for recurrent attention in a sequence decoder.\n",
    "        The callable is a recurrent function of the form:\n",
    "        AttentionState = attend(AttentionInput, AttentionState).\n",
    "\n",
    "        :param source: Shape: (batch_size, seq_len, encoder_num_hidden).\n",
    "        :param source_length: Shape: (batch_size,).\n",
    "        :param source_seq_len: Maximum length of source sequences.\n",
    "        :return: Attention callable.\n",
    "        \"\"\"\n",
    "\n",
    "        coverage_func = self.coverage.on(source, source_length, source_seq_len) if self.coverage else None\n",
    "\n",
    "        # (batch_size, seq_len, attention_num_hidden)\n",
    "        source_hidden = mx.sym.FullyConnected(data=source,\n",
    "                                              weight=self.att_e2h_weight,\n",
    "                                              num_hidden=self.attention_num_hidden,\n",
    "                                              no_bias=True,\n",
    "                                              flatten=False,\n",
    "                                              name=\"%ssource_hidden_fc\" % self.prefix)\n",
    "\n",
    "        def attend(att_input: AttentionInput, att_state: AttentionState) -> AttentionState:\n",
    "            \"\"\"\n",
    "            Returns updated attention state given attention input and current attention state.\n",
    "\n",
    "            :param att_input: Attention input as returned by make_input().\n",
    "            :param att_state: Current attention state\n",
    "            :return: Updated attention state.\n",
    "            \"\"\"\n",
    "\n",
    "            # (batch_size, attention_num_hidden)\n",
    "            query_hidden = mx.sym.FullyConnected(data=att_input.query,\n",
    "                                                 weight=self.att_q2h_weight,\n",
    "                                                 num_hidden=self.attention_num_hidden,\n",
    "                                                 no_bias=True,\n",
    "                                                 name=\"%squery_hidden\" % self.prefix)\n",
    "\n",
    "            # (batch_size, 1, attention_num_hidden)\n",
    "            query_hidden = mx.sym.expand_dims(data=query_hidden,\n",
    "                                              axis=1,\n",
    "                                              name=\"%squery_hidden_expanded\" % self.prefix)\n",
    "\n",
    "            attention_hidden_lhs = source_hidden\n",
    "            if self.coverage:\n",
    "                # (batch_size, seq_len, attention_num_hidden)\n",
    "                dynamic_hidden = mx.sym.FullyConnected(data=att_state.dynamic_source,\n",
    "                                                       weight=self.att_c2h_weight,\n",
    "                                                       num_hidden=self.attention_num_hidden,\n",
    "                                                       no_bias=True,\n",
    "                                                       flatten=False,\n",
    "                                                       name=\"%sdynamic_source_hidden_fc\" % self.prefix)\n",
    "\n",
    "                # (batch_size, seq_len, attention_num_hidden\n",
    "                attention_hidden_lhs = dynamic_hidden + source_hidden\n",
    "\n",
    "            # (batch_size, seq_len, attention_num_hidden)\n",
    "            attention_hidden = mx.sym.broadcast_add(lhs=attention_hidden_lhs, rhs=query_hidden,\n",
    "                                                    name=\"%squery_plus_input\" % self.prefix)\n",
    "\n",
    "            # (batch_size * seq_len, attention_num_hidden)\n",
    "            attention_hidden = mx.sym.reshape(data=attention_hidden,\n",
    "                                              shape=(-3, -1),\n",
    "                                              name=\"%squery_plus_input_before_fc\" % self.prefix)\n",
    "\n",
    "            if self._ln is not None:\n",
    "                attention_hidden = self._ln.normalize(attention_hidden)\n",
    "\n",
    "            # (batch_size * seq_len, attention_num_hidden)\n",
    "            attention_hidden = mx.sym.Activation(attention_hidden, act_type=\"tanh\",\n",
    "                                                 name=\"%shidden\" % self.prefix)\n",
    "\n",
    "            # (batch_size * seq_len, 1)\n",
    "            attention_scores = mx.sym.FullyConnected(data=attention_hidden,\n",
    "                                                     weight=self.att_h2s_weight,\n",
    "                                                     num_hidden=1,\n",
    "                                                     no_bias=True,\n",
    "                                                     name=\"%sraw_att_score_fc\" % self.prefix)\n",
    "\n",
    "            # (batch_size, seq_len, 1)\n",
    "            attention_scores = mx.sym.reshape(attention_scores,\n",
    "                                              shape=(-1, source_seq_len, 1),\n",
    "                                              name=\"%sraw_att_score_fc\" % self.prefix)\n",
    "\n",
    "            context, attention_probs = get_context_and_attention_probs(source, source_length, attention_scores)\n",
    "\n",
    "            dynamic_source = att_state.dynamic_source\n",
    "            if self.coverage:\n",
    "                # update dynamic source encoding\n",
    "                # Note: this is a slight change to the Tu et al, 2016 paper: input to the coverage update\n",
    "                # is the attention input query, not the previous decoder state.\n",
    "                dynamic_source = coverage_func(prev_hidden=att_input.query,\n",
    "                                               attention_prob_scores=attention_probs,\n",
    "                                               prev_coverage=att_state.dynamic_source)\n",
    "\n",
    "            return AttentionState(context=context,\n",
    "                                  probs=attention_probs,\n",
    "                                  dynamic_source=dynamic_source)\n",
    "\n",
    "        return attend\n",
    "\n",
    "\n",
    "def mask_attention_scores(logits: mx.sym.Symbol,\n",
    "                          length: mx.sym.Symbol) -> mx.sym.Symbol:\n",
    "    \"\"\"\n",
    "    Masks attention scores according to sequence length.\n",
    "\n",
    "    :param logits: Shape: (batch_size, seq_len, 1).\n",
    "    :param length: Shape: (batch_size,).\n",
    "    :return: Masked logits: (batch_size, seq_len, 1).\n",
    "    \"\"\"\n",
    "    # TODO: Masking with 0-1 mask, to avoid the multiplication\n",
    "    logits = mx.sym.swapaxes(data=logits, dim1=0, dim2=1)\n",
    "    logits = mx.sym.SequenceMask(data=logits,\n",
    "                                 use_sequence_length=True,\n",
    "                                 sequence_length=length,\n",
    "                                 value=-99999999.)\n",
    "    # (batch_size, seq_len, 1)\n",
    "    return mx.sym.swapaxes(data=logits, dim1=0, dim2=1)\n",
    "\n",
    "\n",
    "def get_context_and_attention_probs(values: mx.sym.Symbol,\n",
    "                                    length: mx.sym.Symbol,\n",
    "                                    logits: mx.sym.Symbol) -> Tuple[mx.sym.Symbol, mx.sym.Symbol]:\n",
    "    \"\"\"\n",
    "    Returns context vector and attention probabilities\n",
    "    via a weighted sum over values.\n",
    "\n",
    "    :param values: Shape: (batch_size, seq_len, encoder_num_hidden).\n",
    "    :param length: Shape: (batch_size,).\n",
    "    :param logits: Shape: (batch_size, seq_len, 1).\n",
    "    :return: context: (batch_size, encoder_num_hidden), attention_probs: (batch_size, seq_len).\n",
    "    \"\"\"\n",
    "    # (batch_size, seq_len, 1)\n",
    "    logits = mask_attention_scores(logits, length)\n",
    "\n",
    "    # (batch_size, seq_len, 1)\n",
    "    probs = mx.sym.softmax(logits, axis=1, name='attention_softmax')\n",
    "\n",
    "    # batch_dot: (batch, M, K) X (batch, K, N) –> (batch, M, N).\n",
    "    # (batch_size, seq_len, num_hidden) X (batch_size, seq_len, 1) -> (batch_size, num_hidden, 1)\n",
    "    context = mx.sym.batch_dot(lhs=values, rhs=probs, transpose_a=True)\n",
    "    # (batch_size, encoder_num_hidden, 1)-> (batch_size, encoder_num_hidden)\n",
    "    context = mx.sym.reshape(data=context, shape=(0, 0))\n",
    "    probs = mx.sym.reshape(data=probs, shape=(0, 0))\n",
    "\n",
    "    return context, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mx.viz.plot_network(encoder(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
