{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TACOTRON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import nd, autograd\n",
    "from IPython.display import clear_output\n",
    "ctx= mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 256\n",
    "\n",
    "emb_size=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab():\n",
    "    vocab = \"abcdefghijklmnopqrstuvwxyz'\" # E: Empty. ignore G\n",
    "    char2idx = {char:idx for idx, char in enumerate(vocab)}\n",
    "    idx2char = {idx:char for idx, char in enumerate(vocab)}\n",
    "    return char2idx, idx2char  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FC-256-ReLU → Dropout(0.5) → FC-128-ReLU → Dropout(0.5)\n",
    "\"\"\"\n",
    "def prenet_pass(data):\n",
    "    fc1 = mx.symbol.FullyConnected(data=data, num_hidden=emb_size)\n",
    "    act1 = mx.symbol.Activation(data=fc1, act_type='relu')\n",
    "    drop1 = mx.symbol.Dropout(act1, p=0.5)\n",
    "    \n",
    "    fc2 = mx.symbol.FullyConnected(data=drop1, num_hidden=emb_size//2)\n",
    "    act2 = mx.symbol.Activation(data=fc2, act_type='relu')\n",
    "    prenet_output = mx.symbol.Dropout(act2, p=0.5)\n",
    "    \n",
    "    return prenet_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# banco di filtri convolutivi. Vengono creati K filtri con kernel 1D di dimensione:k \n",
    "def conv1dBank(conv_input, K):\n",
    "    conv=mx.sym.Convolution(data=conv_input, kernel=(1,1), num_filter=emb_size//2)\n",
    "    (conv, mean, var) = mx.sym.BatchNorm(data=conv, output_mean_var=True)\n",
    "    conv = mx.sym.Activation(data=conv, act_type='relu')\n",
    "    for k in range(2, K+1):\n",
    "        convi = mx.sym.Convolution(data=conv_input, kernel=(k,1), num_filter=emb_size//2)\n",
    "        (convi, mean, var) = mx.sym.BatchNorm(data=convi, output_mean_var=True)\n",
    "        convi = mx.sym.Activation(data=convi, act_type='relu')\n",
    "        conv = mx.symbol.concat(conv,convi)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# highway\n",
    "def highway_layer(data):\n",
    "    H= mx.symbol.Activation(data=mx.symbol.FullyConnected(data=data, num_hidden=emb_size//2), act_type=\"relu\")\n",
    "    T= mx.symbol.Activation(data=mx.symbol.FullyConnected(data=data, num_hidden=emb_size//2, bias=mx.sym.Variable('bias') ), act_type=\"sigmoid\")\n",
    "    return  H * T + data * (1.0 - T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CBHG\n",
    "def CBHG(data,K,proj1_size,proj2_size):\n",
    "    #se si usa infer_shape su convbank dando la dimensione dell'input, viene dedotta la shape appunto \n",
    "    bank = conv1dBank(data,K)\n",
    "    poold_bank = mx.sym.Pooling(data=bank, pool_type='max', kernel=(2, 1), stride=(1,1))\n",
    "\n",
    "    proj1 = mx.sym.Convolution(data=poold_bank, kernel=(3,1), num_filter=proj1_size)\n",
    "    (proj1, proj1_mean, proj1_var) = mx.sym.BatchNorm(data=proj1, output_mean_var=True)\n",
    "    proj1 = mx.sym.Activation(data=proj1, act_type='relu')#Is it ok to declare/use again the same variable?  \n",
    "\n",
    "    proj2 = mx.sym.Convolution(proj1, kernel=(3,1), num_filter=proj2_size)\n",
    "    (proj2, proj2_mean, proj2_var) = mx.sym.BatchNorm(data=proj2, output_mean_var=True)\n",
    "    \n",
    "    residual= proj2 + data\n",
    "\n",
    "    for i in range(4):\n",
    "        residual = highway_layer(residual)\n",
    "    highway_pass = residual\n",
    "   \n",
    "    bidirectional_gru_cell = mx.rnn.BidirectionalCell(mx.rnn.GRUCell(num_hidden=emb_size//2),\n",
    "                                                      mx.rnn.GRUCell(num_hidden=emb_size//2)\n",
    "                                                     )\n",
    "    outputs, states = bidirectional_gru_cell.unroll(1, inputs=highway_pass, merge_outputs=True)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "def encoder(data):\n",
    "    char2index, index2char = load_vocab()\n",
    "    \n",
    "    onehot = mx.sym.one_hot(data,len(char2index))\n",
    "    embed_vector = mx.sym.Embedding(data=onehot, input_dim=100, output_dim=emb_size)\n",
    "    prenet_output = prenet_pass(embed_vector)\n",
    "    return CBHG(prenet_output,16, emb_size//2, emb_size//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'encoder.gv.pdf'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = mx.sym.Variable('text')\n",
    "\n",
    "encoded = encoder(text)\n",
    "graph=mx.viz.plot_network(\n",
    "    encoded,\n",
    "    save_format='pdf',\n",
    "    title='encoder')\n",
    "graph.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder\n",
    "def decoder(data,context,reduction_factor):\n",
    "    prenet_output = prenet_pass(data)\n",
    "        \n",
    "    stack = mx.rnn.SequentialRNNCell()\n",
    "    stack.add(mx.rnn.GRUCell(num_hidden=emb_size,prefix='decoder_layer1_'))\n",
    "    stack.add(mx.rnn.GRUCell(num_hidden=emb_size,prefix='decoder_layer2_'))\n",
    "    \n",
    "    residual_gru_stack = mx.rnn.ResidualCell(stack)\n",
    "    \n",
    "    gru_outputs,states = residual_gru_stack.unroll(length=1,\n",
    "                                               inputs=prenet_output,\n",
    "                                               begin_state=context,\n",
    "                                               merge_outputs=True)\n",
    "\n",
    "    predicted_frames = mx.symbol.Activation(\n",
    "        data=mx.symbol.FullyConnected(data=gru_outputs, num_hidden=reduction_factor),\n",
    "        act_type=\"relu\"\n",
    "    )\n",
    "    \n",
    "    return predicted_frames, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decoder.gv.pdf'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectrogram = mx.sym.Variable('spectrogram')\n",
    "reduction_factor=2\n",
    "spectrograms_count=1 #dummy value\n",
    "decoder_state=[encoded,encoded]\n",
    "predicted_frames=spectrogram\n",
    "full_frame=mx.sym.Variable('go')\n",
    "\n",
    "for i in range(spectrograms_count):\n",
    "    predicted_frames,decoder_state = decoder(predicted_frames,decoder_state,reduction_factor)\n",
    "    full_frame=mx.sym.concat(full_frame,predicted_frames)\n",
    "\n",
    "spectral_magnitude=CBHG(full_frame,8, emb_size, 80)\n",
    "\n",
    "graph=mx.viz.plot_network(\n",
    "    full_frame,\n",
    "    save_format='pdf',\n",
    "    title='decoder')\n",
    "graph.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2> Attention model part </h2>\n",
    "\n",
    "Need to figure out what is going on here.\n",
    "\n",
    "\"memory\" part = data (embeddings, apples, whatever)-> prenet -> encoder-CBHG   \n",
    "\n",
    "keithito implementation: \n",
    "\n",
    "      # Attention\n",
    "      attention_cell = AttentionWrapper(\n",
    "        DecoderPrenetWrapper(GRUCell(256), is_training),\n",
    "        BahdanauAttention(256, encoder_outputs),\n",
    "        alignment_history=True,\n",
    "        output_attention=False)                                                  # [N, T_in, 256]\n",
    "first arg: a RNN cell \n",
    "second: attention mechanism\n",
    "\n",
    "both AttentionWrapper and BahdanauAttention comes from tf.contrib.seq2seq package\n",
    "    \n",
    "The DecoderPrenetWrapper:\n",
    "<div style=\"background-color:gray\">\n",
    "<code style=\"background-color:gray\">\n",
    "class DecoderPrenetWrapper(RNNCell):\n",
    "  '''Runs RNN inputs through a prenet before sending them to the cell.'''\n",
    "  \n",
    "  bla bla bla\n",
    "  \n",
    "  <b>def call(self, inputs, state):\n",
    "    prenet_out = prenet(inputs, self._is_training, scope='decoder_prenet')\n",
    "    return self._cell(prenet_out, state)\n",
    "  </b>  \n",
    "  bla bla bla\n",
    "</code></div>\n",
    "\n",
    "So it just send data to the prenet. Data comes from encoder_cbhg.\n",
    "    The AttentionWrapper, wraps a attention mechanism (Bahdanau) that look at a memory to query (the encoder output) and a net (GRU 256) that is the attention net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
